{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1265,
     "status": "ok",
     "timestamp": 1743011451495,
     "user": {
      "displayName": "–®–∞–¥–∞–µ–≤ –§—ë–¥–æ—Ä",
      "userId": "14152552676221368510"
     },
     "user_tz": -180
    },
    "id": "hyWB8OG2DiFu"
   },
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4129,
     "status": "ok",
     "timestamp": 1743011445965,
     "user": {
      "displayName": "–®–∞–¥–∞–µ–≤ –§—ë–¥–æ—Ä",
      "userId": "14152552676221368510"
     },
     "user_tz": -180
    },
    "id": "Xw3c6P7BkP9b",
    "outputId": "0754cfd2-1245-4d72-b3d3-4b79cc30e68b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshadaevf\u001b[0m (\u001b[33mshadaevf-rtu-mirea\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "%wandb login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sye8ukc4STIc"
   },
   "source": [
    "# –ß–∞—Å—Ç—å –ø–µ—Ä–≤–∞—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IiuvNyEsOqz9"
   },
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \"\"\" stores a single scalar value and its gradient \"\"\"\n",
    "\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0\n",
    "        # internal variables used for autograd graph construction\n",
    "        self._backward = lambda: None # function\n",
    "        self._prev = set(_children) # set of Value objects\n",
    "        self._op = _op # the op that produced this node, string ('+', '-', ....)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, _op=\"+\", _children=(self, other))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, _op=\"*\", _children=(self, other))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad * other.data\n",
    "            other.grad += out.grad * self.data\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data ** other, _op=\"**\", _children=(self, ))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad * (other * ((self.data) ** (other-1)))\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        out = Value(self.data if self.data > 0 else 0, _op=\"relu\", _children=(self, ))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad * (1 if self.data > 0 else 0)\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        # topological order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "    def __neg__(self): # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other): # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        return other * self**-1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hVBLsqj1UjQN"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Module:\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for parameter in self.parameters():\n",
    "          parameter.grad=0\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Neuron(Module):\n",
    "\n",
    "    def __init__(self, nin: int, nonlin=True):\n",
    "        self.w = [Value(random.normalvariate(0, 2 / nin)) for _ in range(nin)]\n",
    "        self.b = Value(random.normalvariate(0, 2 / nin))\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "    def __call__(self, x):\n",
    "        act = 0\n",
    "        for i, el in enumerate(x):\n",
    "          act += el * self.w[i]\n",
    "        act += self.b\n",
    "        return act.relu() if self.nonlin else act\n",
    "\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{'ReLU' if self.nonlin else 'Linear'}Neuron({len(self.w)})\"\n",
    "\n",
    "class Layer(Module):\n",
    "\n",
    "    def __init__(self, nin, nout, **kwargs):\n",
    "        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = []\n",
    "        for neuron in self.neurons:\n",
    "          out.append(neuron(x))\n",
    "        return out[0] if len(out) == 1 else out\n",
    "\n",
    "    def parameters(self):\n",
    "        out = []\n",
    "        for neuron in self.neurons:\n",
    "          out += neuron.parameters()\n",
    "        return out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3K8DQXHXUoBG"
   },
   "outputs": [],
   "source": [
    "def train(model, y, X, learning_rate=0.001, epochs=10, show=1):\n",
    "    for k in range(epochs):\n",
    "\n",
    "        # forward\n",
    "        y_pred = []\n",
    "        for row in X:\n",
    "          y_pred.append(model(row))\n",
    "\n",
    "        # calculate loss (mean square error)\n",
    "        loss = 0\n",
    "        for i in range(len(y_pred)):\n",
    "          for j in range(len(y_pred[0])):\n",
    "            loss += (y_pred[i][j] - y[i][j]) ** 2\n",
    "        loss /= len(y_pred) * len(y_pred)\n",
    "\n",
    "        # backward (zero_grad + backward)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # update\n",
    "        for p in model.parameters():\n",
    "          p.data -= learning_rate * p.grad\n",
    "\n",
    "        if k % show == 0:\n",
    "            print(f\"step {k} loss {loss.data}\")\n",
    "\n",
    "        wandb.log({\"loss\": loss.data})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKQ4KLKtWnfS"
   },
   "outputs": [],
   "source": [
    "model = Layer(3, 2)\n",
    "\n",
    "xs = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0]\n",
    "]\n",
    "ys = [[1.0, 1.0], [-1.0, 1.0], [-1.0, -1.0], [-1.0, 1.0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 74,
     "status": "error",
     "timestamp": 1743007863924,
     "user": {
      "displayName": "–®–∞–¥–∞–µ–≤ –§—ë–¥–æ—Ä",
      "userId": "14152552676221368510"
     },
     "user_tz": -180
    },
    "id": "qpYa4lX7WqQ4",
    "outputId": "3af4c91b-3e3c-4c19-b03d-d50275d3d05f"
   },
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "      # Set the project where this run will be logged\n",
    "      project=\"ml_pr_2\",\n",
    "      # We pass a run name (otherwise it‚Äôll be randomly assigned, like sunshine-lollypop-10)\n",
    "      name=\"myautograd\"\n",
    "      )\n",
    "\n",
    "train(model, ys, xs, learning_rate=0.3, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAIAX_wVFl_L"
   },
   "source": [
    "# –ß–∞—Å—Ç—å 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4605,
     "status": "ok",
     "timestamp": 1743011463197,
     "user": {
      "displayName": "–®–∞–¥–∞–µ–≤ –§—ë–¥–æ—Ä",
      "userId": "14152552676221368510"
     },
     "user_tz": -180
    },
    "id": "LLIey57SfXFt"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from ray import tune\n",
    "from ray import train\n",
    "from ray.train import Checkpoint, get_checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import ray.cloudpickle as pickle\n",
    "from ray.air.integrations.wandb import WandbLoggerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1742999556939,
     "user": {
      "displayName": "–®–∞–¥–∞–µ–≤ –§—ë–¥–æ—Ä",
      "userId": "14152552676221368510"
     },
     "user_tz": -180
    },
    "id": "b0X8sJOfwTTO",
    "outputId": "12053c0c-dbae-4a2d-b7a0-529829531dec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1742999556961,
     "user": {
      "displayName": "–®–∞–¥–∞–µ–≤ –§—ë–¥–æ—Ä",
      "userId": "14152552676221368510"
     },
     "user_tz": -180
    },
    "id": "-MrDOMjFwaH_",
    "outputId": "0b60da8f-84d7-42e2-a40e-b9115ddc0612"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1743011471120,
     "user": {
      "displayName": "–®–∞–¥–∞–µ–≤ –§—ë–¥–æ—Ä",
      "userId": "14152552676221368510"
     },
     "user_tz": -180
    },
    "id": "I6le8-4OI0nL"
   },
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "def load_data(data_dir=\"./data\"):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5), (0.5))]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.EMNIST(\n",
    "        root=data_dir, split='letters', train=True, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    testset = torchvision.datasets.EMNIST(\n",
    "        root=data_dir, split='letters', train=False, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1743012782097,
     "user": {
      "displayName": "–®–∞–¥–∞–µ–≤ –§—ë–¥–æ—Ä",
      "userId": "14152552676221368510"
     },
     "user_tz": -180
    },
    "id": "hAEzuM3Ugoc9"
   },
   "outputs": [],
   "source": [
    "# –î–µ–ª–∞–µ–º –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 27)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1743011478054,
     "user": {
      "displayName": "–®–∞–¥–∞–µ–≤ –§—ë–¥–æ—Ä",
      "userId": "14152552676221368510"
     },
     "user_tz": -180
    },
    "id": "_NgS6c3ah2hb"
   },
   "outputs": [],
   "source": [
    "def train_emnist(config, data_dir=None):\n",
    "  # –°–æ–∑–¥–∞—ë–º —Å–µ—Ç—å\n",
    "  net = NeuralNetwork()\n",
    "  # –í—ã–±–∏—Ä–∞–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
    "  device = \"cpu\"\n",
    "  if torch.cuda.is_available():\n",
    "      device = \"cuda\"\n",
    "  net.to(device)\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
    "  # –ï—Å–ª–∏ —É–∂–µ –±—ã–ª–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫–∞—è-—Ç–æ –º–æ–¥–µ–ª—å, —Ç–æ –¥–æ—Å—Ç–∞—ë–º –µ—ë –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ç—é–Ω–∏—Ç—å\n",
    "  checkpoint = get_checkpoint()\n",
    "  if checkpoint:\n",
    "      with checkpoint.as_directory() as checkpoint_dir:\n",
    "          data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "          with open(data_path, \"rb\") as fp:\n",
    "              checkpoint_state = pickle.load(fp)\n",
    "          start_epoch = checkpoint_state[\"epoch\"]\n",
    "          net.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
    "          optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "  else:\n",
    "      start_epoch = 0\n",
    "\n",
    "  trainset, testset = load_data(data_dir)\n",
    "\n",
    "  val_size = int(len(trainset) * 0.2)\n",
    "  train_subset, val_subset = random_split(\n",
    "      trainset, [len(trainset) - val_size, val_size]\n",
    "  )\n",
    "\n",
    "  trainloader = torch.utils.data.DataLoader(\n",
    "      train_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=2\n",
    "  )\n",
    "  valloader = torch.utils.data.DataLoader(\n",
    "      val_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=2\n",
    "  )\n",
    "\n",
    "  for epoch in range(start_epoch, 10):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(\n",
    "                    \"[%d, %5d] loss: %.3f\"\n",
    "                    % (epoch + 1, i + 1, running_loss / epoch_steps)\n",
    "                )\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        checkpoint_data = {\n",
    "            \"epoch\": epoch,\n",
    "            \"net_state_dict\": net.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }\n",
    "        with tempfile.TemporaryDirectory() as checkpoint_dir:\n",
    "            data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "            with open(data_path, \"wb\") as fp:\n",
    "                pickle.dump(checkpoint_data, fp)\n",
    "\n",
    "            checkpoint = Checkpoint.from_directory(checkpoint_dir)\n",
    "            train.report(\n",
    "                {\"loss\": val_loss / val_steps, \"accuracy\": correct / total},\n",
    "                checkpoint=checkpoint,\n",
    "            )\n",
    "\n",
    "  print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1742999557346,
     "user": {
      "displayName": "–®–∞–¥–∞–µ–≤ –§—ë–¥–æ—Ä",
      "userId": "14152552676221368510"
     },
     "user_tz": -180
    },
    "id": "z7o5NX-OvZo5"
   },
   "outputs": [],
   "source": [
    "# –ü–∏—à–µ–º —Ç–µ—Å—Ç\n",
    "def test_accuracy(net, device=\"cpu\"):\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False, num_workers=2\n",
    "    )\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2300482,
     "status": "ok",
     "timestamp": 1743001857844,
     "user": {
      "displayName": "–®–∞–¥–∞–µ–≤ –§—ë–¥–æ—Ä",
      "userId": "14152552676221368510"
     },
     "user_tz": -180
    },
    "id": "CVkVUbTZy19m",
    "outputId": "a13b3df6-3918-430f-82df-83874fae9370"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 562M/562M [01:14<00:00, 7.51MB/s]\n",
      "2025-03-26 14:34:25,213\tINFO worker.py:1852 -- Started a local Ray instance.\n",
      "2025-03-26 14:34:26,538\tINFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.\n",
      "2025-03-26 14:34:26,597\tINFO tensorboardx.py:193 -- pip install \"ray[tune]\" to see TensorBoard files.\n",
      "2025-03-26 14:34:26,600\tWARNING callback.py:136 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n",
      "2025-03-26 14:34:26,613\tINFO wandb.py:318 -- Already logged into W&B.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------+\n",
      "| Configuration for experiment     train_emnist_2025-03-26_14-34-26   |\n",
      "+---------------------------------------------------------------------+\n",
      "| Search algorithm                 BasicVariantGenerator              |\n",
      "| Scheduler                        AsyncHyperBandScheduler            |\n",
      "| Number of trials                 9                                  |\n",
      "+---------------------------------------------------------------------+\n",
      "\n",
      "View detailed results here: /root/ray_results/train_emnist_2025-03-26_14-34-26\n",
      "\n",
      "Trial status: 9 PENDING\n",
      "Current time: 2025-03-26 14:34:26. Total running time: 0s\n",
      "Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------+\n",
      "| Trial name                 status        lr     batch_size |\n",
      "+------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00000   PENDING    0.1               16 |\n",
      "| train_emnist_6b36b_00001   PENDING    0.1               32 |\n",
      "| train_emnist_6b36b_00002   PENDING    0.1               64 |\n",
      "| train_emnist_6b36b_00003   PENDING    0.01              16 |\n",
      "| train_emnist_6b36b_00004   PENDING    0.01              32 |\n",
      "| train_emnist_6b36b_00005   PENDING    0.01              64 |\n",
      "| train_emnist_6b36b_00006   PENDING    0.001             16 |\n",
      "| train_emnist_6b36b_00007   PENDING    0.001             32 |\n",
      "| train_emnist_6b36b_00008   PENDING    0.001             64 |\n",
      "+------------------------------------------------------------+\n",
      "\n",
      "Trial train_emnist_6b36b_00000 started with configuration:\n",
      "+-----------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00000 config         |\n",
      "+-----------------------------------------------+\n",
      "| batch_size                                 16 |\n",
      "| lr                                        0.1 |\n",
      "+-----------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb: Currently logged in as: shadaevf (shadaevf-rtu-mirea) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb: Tracking run with wandb version 0.19.8\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2025-03-26_14-34-21_606425_267/artifacts/2025-03-26_14-34-26/train_emnist_2025-03-26_14-34-26/driver_artifacts/train_emnist_6b36b_00000_0_batch_size=16,lr=0.1000_2025-03-26_14-34-26/wandb/run-20250326_143435-6b36b_00000\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb: Syncing run train_emnist_6b36b_00000\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb: ‚≠êÔ∏è View project at https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb: üöÄ View run at https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2/runs/6b36b_00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=5148)\u001b[0m [1,  2000] loss: 3.267\n",
      "\u001b[36m(func pid=5148)\u001b[0m [1,  4000] loss: 1.636\n",
      "\n",
      "Trial status: 1 RUNNING | 8 PENDING\n",
      "Current time: 2025-03-26 14:34:56. Total running time: 30s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------+\n",
      "| Trial name                 status        lr     batch_size |\n",
      "+------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00000   RUNNING    0.1               16 |\n",
      "| train_emnist_6b36b_00001   PENDING    0.1               32 |\n",
      "| train_emnist_6b36b_00002   PENDING    0.1               64 |\n",
      "| train_emnist_6b36b_00003   PENDING    0.01              16 |\n",
      "| train_emnist_6b36b_00004   PENDING    0.01              32 |\n",
      "| train_emnist_6b36b_00005   PENDING    0.01              64 |\n",
      "| train_emnist_6b36b_00006   PENDING    0.001             16 |\n",
      "| train_emnist_6b36b_00007   PENDING    0.001             32 |\n",
      "| train_emnist_6b36b_00008   PENDING    0.001             64 |\n",
      "+------------------------------------------------------------+\n",
      "\u001b[36m(func pid=5148)\u001b[0m [1,  6000] loss: 1.091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=5148)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00000_0_batch_size=16,lr=0.1000_2025-03-26_14-34-26/checkpoint_000000)\n",
      "2025-03-26 14:35:14,503\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.219 s, which may be a performance bottleneck.\n",
      "2025-03-26 14:35:14,505\tWARNING util.py:201 -- The `process_trial_result` operation took 1.222 s, which may be a performance bottleneck.\n",
      "2025-03-26 14:35:14,507\tWARNING util.py:201 -- Processing trial results took 1.224 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-03-26 14:35:14,507\tWARNING util.py:201 -- The `process_trial_result` operation took 1.224 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial train_emnist_6b36b_00000 finished iteration 1 at 2025-03-26 14:35:14. Total running time: 47s\n",
      "+-------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00000 result                       |\n",
      "+-------------------------------------------------------------+\n",
      "| checkpoint_dir_name                       checkpoint_000000 |\n",
      "| time_this_iter_s                                    41.6215 |\n",
      "| time_total_s                                        41.6215 |\n",
      "| training_iteration                                        1 |\n",
      "| accuracy                                             0.0369 |\n",
      "| loss                                      3.273698568344116 |\n",
      "+-------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00000 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00000_0_batch_size=16,lr=0.1000_2025-03-26_14-34-26/checkpoint_000000\n",
      "\u001b[36m(func pid=5148)\u001b[0m [2,  2000] loss: 3.274\n",
      "\n",
      "Trial status: 1 RUNNING | 8 PENDING\n",
      "Current time: 2025-03-26 14:35:27. Total running time: 1min 0s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status        lr     batch_size     iter     total time (s)     loss     accuracy |\n",
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00000   RUNNING    0.1               16        1            41.6215   3.2737     0.036899 |\n",
      "| train_emnist_6b36b_00001   PENDING    0.1               32                                                   |\n",
      "| train_emnist_6b36b_00002   PENDING    0.1               64                                                   |\n",
      "| train_emnist_6b36b_00003   PENDING    0.01              16                                                   |\n",
      "| train_emnist_6b36b_00004   PENDING    0.01              32                                                   |\n",
      "| train_emnist_6b36b_00005   PENDING    0.01              64                                                   |\n",
      "| train_emnist_6b36b_00006   PENDING    0.001             16                                                   |\n",
      "| train_emnist_6b36b_00007   PENDING    0.001             32                                                   |\n",
      "| train_emnist_6b36b_00008   PENDING    0.001             64                                                   |\n",
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=5148)\u001b[0m [2,  4000] loss: 1.637\n",
      "\u001b[36m(func pid=5148)\u001b[0m [2,  6000] loss: 1.091\n",
      "\n",
      "Trial train_emnist_6b36b_00000 finished iteration 2 at 2025-03-26 14:35:52. Total running time: 1min 25s\n",
      "+-------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00000 result                       |\n",
      "+-------------------------------------------------------------+\n",
      "| checkpoint_dir_name                       checkpoint_000001 |\n",
      "| time_this_iter_s                                   37.68491 |\n",
      "| time_total_s                                       79.30641 |\n",
      "| training_iteration                                        2 |\n",
      "| accuracy                                            0.03694 |\n",
      "| loss                                      3.269702434539795 |\n",
      "+-------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00000 saved a checkpoint for iteration 2 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00000_0_batch_size=16,lr=0.1000_2025-03-26_14-34-26/checkpoint_000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=5148)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00000_0_batch_size=16,lr=0.1000_2025-03-26_14-34-26/checkpoint_000001)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 1 RUNNING | 8 PENDING\n",
      "Current time: 2025-03-26 14:35:57. Total running time: 1min 30s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status        lr     batch_size     iter     total time (s)     loss     accuracy |\n",
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00000   RUNNING    0.1               16        2            79.3064   3.2697    0.0369391 |\n",
      "| train_emnist_6b36b_00001   PENDING    0.1               32                                                   |\n",
      "| train_emnist_6b36b_00002   PENDING    0.1               64                                                   |\n",
      "| train_emnist_6b36b_00003   PENDING    0.01              16                                                   |\n",
      "| train_emnist_6b36b_00004   PENDING    0.01              32                                                   |\n",
      "| train_emnist_6b36b_00005   PENDING    0.01              64                                                   |\n",
      "| train_emnist_6b36b_00006   PENDING    0.001             16                                                   |\n",
      "| train_emnist_6b36b_00007   PENDING    0.001             32                                                   |\n",
      "| train_emnist_6b36b_00008   PENDING    0.001             64                                                   |\n",
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=5148)\u001b[0m [3,  2000] loss: 3.272\n",
      "\u001b[36m(func pid=5148)\u001b[0m [3,  4000] loss: 1.637\n",
      "\u001b[36m(func pid=5148)\u001b[0m [3,  6000] loss: 1.091\n",
      "Trial status: 1 RUNNING | 8 PENDING\n",
      "Current time: 2025-03-26 14:36:27. Total running time: 2min 0s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status        lr     batch_size     iter     total time (s)     loss     accuracy |\n",
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00000   RUNNING    0.1               16        2            79.3064   3.2697    0.0369391 |\n",
      "| train_emnist_6b36b_00001   PENDING    0.1               32                                                   |\n",
      "| train_emnist_6b36b_00002   PENDING    0.1               64                                                   |\n",
      "| train_emnist_6b36b_00003   PENDING    0.01              16                                                   |\n",
      "| train_emnist_6b36b_00004   PENDING    0.01              32                                                   |\n",
      "| train_emnist_6b36b_00005   PENDING    0.01              64                                                   |\n",
      "| train_emnist_6b36b_00006   PENDING    0.001             16                                                   |\n",
      "| train_emnist_6b36b_00007   PENDING    0.001             32                                                   |\n",
      "| train_emnist_6b36b_00008   PENDING    0.001             64                                                   |\n",
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_emnist_6b36b_00000 finished iteration 3 at 2025-03-26 14:36:29. Total running time: 2min 2s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00000 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000002 |\n",
      "| time_this_iter_s                                    36.82944 |\n",
      "| time_total_s                                       116.13585 |\n",
      "| training_iteration                                         3 |\n",
      "| accuracy                                              0.0389 |\n",
      "| loss                                      3.2727580070495605 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00000 saved a checkpoint for iteration 3 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00000_0_batch_size=16,lr=0.1000_2025-03-26_14-34-26/checkpoint_000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=5148)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00000_0_batch_size=16,lr=0.1000_2025-03-26_14-34-26/checkpoint_000002)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=5148)\u001b[0m [4,  2000] loss: 3.274\n",
      "\u001b[36m(func pid=5148)\u001b[0m [4,  4000] loss: 1.637\n",
      "\n",
      "Trial status: 1 RUNNING | 8 PENDING\n",
      "Current time: 2025-03-26 14:36:57. Total running time: 2min 30s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status        lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00000   RUNNING    0.1               16        3            116.136   3.27276    0.0389022 |\n",
      "| train_emnist_6b36b_00001   PENDING    0.1               32                                                    |\n",
      "| train_emnist_6b36b_00002   PENDING    0.1               64                                                    |\n",
      "| train_emnist_6b36b_00003   PENDING    0.01              16                                                    |\n",
      "| train_emnist_6b36b_00004   PENDING    0.01              32                                                    |\n",
      "| train_emnist_6b36b_00005   PENDING    0.01              64                                                    |\n",
      "| train_emnist_6b36b_00006   PENDING    0.001             16                                                    |\n",
      "| train_emnist_6b36b_00007   PENDING    0.001             32                                                    |\n",
      "| train_emnist_6b36b_00008   PENDING    0.001             64                                                    |\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=5148)\u001b[0m [4,  6000] loss: 1.091\n",
      "\n",
      "Trial train_emnist_6b36b_00000 finished iteration 4 at 2025-03-26 14:37:06. Total running time: 2min 40s\n",
      "+-------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00000 result                       |\n",
      "+-------------------------------------------------------------+\n",
      "| checkpoint_dir_name                       checkpoint_000003 |\n",
      "| time_this_iter_s                                   37.56149 |\n",
      "| time_total_s                                      153.69733 |\n",
      "| training_iteration                                        4 |\n",
      "| accuracy                                            0.03694 |\n",
      "| loss                                      3.265331983566284 |\n",
      "+-------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00000 saved a checkpoint for iteration 4 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00000_0_batch_size=16,lr=0.1000_2025-03-26_14-34-26/checkpoint_000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=5148)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00000_0_batch_size=16,lr=0.1000_2025-03-26_14-34-26/checkpoint_000003)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=5148)\u001b[0m [5,  2000] loss: 3.273\n",
      "\u001b[36m(func pid=5148)\u001b[0m [5,  4000] loss: 1.636\n",
      "\n",
      "Trial status: 1 RUNNING | 8 PENDING\n",
      "Current time: 2025-03-26 14:37:27. Total running time: 3min 0s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status        lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00000   RUNNING    0.1               16        4            153.697   3.26533    0.0369391 |\n",
      "| train_emnist_6b36b_00001   PENDING    0.1               32                                                    |\n",
      "| train_emnist_6b36b_00002   PENDING    0.1               64                                                    |\n",
      "| train_emnist_6b36b_00003   PENDING    0.01              16                                                    |\n",
      "| train_emnist_6b36b_00004   PENDING    0.01              32                                                    |\n",
      "| train_emnist_6b36b_00005   PENDING    0.01              64                                                    |\n",
      "| train_emnist_6b36b_00006   PENDING    0.001             16                                                    |\n",
      "| train_emnist_6b36b_00007   PENDING    0.001             32                                                    |\n",
      "| train_emnist_6b36b_00008   PENDING    0.001             64                                                    |\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=5148)\u001b[0m [5,  6000] loss: 1.091\n",
      "\n",
      "Trial train_emnist_6b36b_00000 finished iteration 5 at 2025-03-26 14:37:44. Total running time: 3min 18s\n",
      "+-------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00000 result                       |\n",
      "+-------------------------------------------------------------+\n",
      "| checkpoint_dir_name                       checkpoint_000004 |\n",
      "| time_this_iter_s                                   38.12206 |\n",
      "| time_total_s                                      191.81939 |\n",
      "| training_iteration                                        5 |\n",
      "| accuracy                                            0.03742 |\n",
      "| loss                                      3.281830072402954 |\n",
      "+-------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00000 saved a checkpoint for iteration 5 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00000_0_batch_size=16,lr=0.1000_2025-03-26_14-34-26/checkpoint_000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=5148)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00000_0_batch_size=16,lr=0.1000_2025-03-26_14-34-26/checkpoint_000004)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=5148)\u001b[0m [6,  2000] loss: 3.274\n",
      "\n",
      "Trial status: 1 RUNNING | 8 PENDING\n",
      "Current time: 2025-03-26 14:37:57. Total running time: 3min 30s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status        lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00000   RUNNING    0.1               16        5            191.819   3.28183    0.0374199 |\n",
      "| train_emnist_6b36b_00001   PENDING    0.1               32                                                    |\n",
      "| train_emnist_6b36b_00002   PENDING    0.1               64                                                    |\n",
      "| train_emnist_6b36b_00003   PENDING    0.01              16                                                    |\n",
      "| train_emnist_6b36b_00004   PENDING    0.01              32                                                    |\n",
      "| train_emnist_6b36b_00005   PENDING    0.01              64                                                    |\n",
      "| train_emnist_6b36b_00006   PENDING    0.001             16                                                    |\n",
      "| train_emnist_6b36b_00007   PENDING    0.001             32                                                    |\n",
      "| train_emnist_6b36b_00008   PENDING    0.001             64                                                    |\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=5148)\u001b[0m [6,  4000] loss: 1.636\n",
      "\u001b[36m(func pid=5148)\u001b[0m [6,  6000] loss: 1.091\n",
      "\n",
      "Trial train_emnist_6b36b_00000 finished iteration 6 at 2025-03-26 14:38:21. Total running time: 3min 55s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00000 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000005 |\n",
      "| time_this_iter_s                                    37.05406 |\n",
      "| time_total_s                                       228.87345 |\n",
      "| training_iteration                                         6 |\n",
      "| accuracy                                             0.03858 |\n",
      "| loss                                      3.2703194618225098 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00000 saved a checkpoint for iteration 6 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00000_0_batch_size=16,lr=0.1000_2025-03-26_14-34-26/checkpoint_000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=5148)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00000_0_batch_size=16,lr=0.1000_2025-03-26_14-34-26/checkpoint_000005)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 1 RUNNING | 8 PENDING\n",
      "Current time: 2025-03-26 14:38:27. Total running time: 4min 0s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status        lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00000   RUNNING    0.1               16        6            228.873   3.27032    0.0385817 |\n",
      "| train_emnist_6b36b_00001   PENDING    0.1               32                                                    |\n",
      "| train_emnist_6b36b_00002   PENDING    0.1               64                                                    |\n",
      "| train_emnist_6b36b_00003   PENDING    0.01              16                                                    |\n",
      "| train_emnist_6b36b_00004   PENDING    0.01              32                                                    |\n",
      "| train_emnist_6b36b_00005   PENDING    0.01              64                                                    |\n",
      "| train_emnist_6b36b_00006   PENDING    0.001             16                                                    |\n",
      "| train_emnist_6b36b_00007   PENDING    0.001             32                                                    |\n",
      "| train_emnist_6b36b_00008   PENDING    0.001             64                                                    |\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=5148)\u001b[0m [7,  2000] loss: 3.273\n",
      "\u001b[36m(func pid=5148)\u001b[0m [7,  4000] loss: 1.636\n",
      "\u001b[36m(func pid=5148)\u001b[0m [7,  6000] loss: 1.091\n",
      "Trial status: 1 RUNNING | 8 PENDING\n",
      "Current time: 2025-03-26 14:38:57. Total running time: 4min 30s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status        lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00000   RUNNING    0.1               16        6            228.873   3.27032    0.0385817 |\n",
      "| train_emnist_6b36b_00001   PENDING    0.1               32                                                    |\n",
      "| train_emnist_6b36b_00002   PENDING    0.1               64                                                    |\n",
      "| train_emnist_6b36b_00003   PENDING    0.01              16                                                    |\n",
      "| train_emnist_6b36b_00004   PENDING    0.01              32                                                    |\n",
      "| train_emnist_6b36b_00005   PENDING    0.01              64                                                    |\n",
      "| train_emnist_6b36b_00006   PENDING    0.001             16                                                    |\n",
      "| train_emnist_6b36b_00007   PENDING    0.001             32                                                    |\n",
      "| train_emnist_6b36b_00008   PENDING    0.001             64                                                    |\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_emnist_6b36b_00000 finished iteration 7 at 2025-03-26 14:38:59. Total running time: 4min 33s\n",
      "+-------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00000 result                       |\n",
      "+-------------------------------------------------------------+\n",
      "| checkpoint_dir_name                       checkpoint_000006 |\n",
      "| time_this_iter_s                                    37.9109 |\n",
      "| time_total_s                                      266.78435 |\n",
      "| training_iteration                                        7 |\n",
      "| accuracy                                            0.04083 |\n",
      "| loss                                       3.26592755317688 |\n",
      "+-------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00000 saved a checkpoint for iteration 7 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00000_0_batch_size=16,lr=0.1000_2025-03-26_14-34-26/checkpoint_000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=5148)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00000_0_batch_size=16,lr=0.1000_2025-03-26_14-34-26/checkpoint_000006)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=5148)\u001b[0m [8,  2000] loss: 3.273\n",
      "\u001b[36m(func pid=5148)\u001b[0m [8,  4000] loss: 1.637\n",
      "\n",
      "Trial status: 1 RUNNING | 8 PENDING\n",
      "Current time: 2025-03-26 14:39:27. Total running time: 5min 0s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status        lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00000   RUNNING    0.1               16        7            266.784   3.26593    0.0408253 |\n",
      "| train_emnist_6b36b_00001   PENDING    0.1               32                                                    |\n",
      "| train_emnist_6b36b_00002   PENDING    0.1               64                                                    |\n",
      "| train_emnist_6b36b_00003   PENDING    0.01              16                                                    |\n",
      "| train_emnist_6b36b_00004   PENDING    0.01              32                                                    |\n",
      "| train_emnist_6b36b_00005   PENDING    0.01              64                                                    |\n",
      "| train_emnist_6b36b_00006   PENDING    0.001             16                                                    |\n",
      "| train_emnist_6b36b_00007   PENDING    0.001             32                                                    |\n",
      "| train_emnist_6b36b_00008   PENDING    0.001             64                                                    |\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=5148)\u001b[0m [8,  6000] loss: 1.091\n",
      "\n",
      "Trial train_emnist_6b36b_00000 finished iteration 8 at 2025-03-26 14:39:37. Total running time: 5min 11s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00000 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000007 |\n",
      "| time_this_iter_s                                    38.07188 |\n",
      "| time_total_s                                       304.85623 |\n",
      "| training_iteration                                         8 |\n",
      "| accuracy                                             0.03722 |\n",
      "| loss                                      3.2737338542938232 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00000 saved a checkpoint for iteration 8 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00000_0_batch_size=16,lr=0.1000_2025-03-26_14-34-26/checkpoint_000007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=5148)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00000_0_batch_size=16,lr=0.1000_2025-03-26_14-34-26/checkpoint_000007)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=5148)\u001b[0m [9,  2000] loss: 3.273\n",
      "\u001b[36m(func pid=5148)\u001b[0m [9,  4000] loss: 1.637\n",
      "\n",
      "Trial status: 1 RUNNING | 8 PENDING\n",
      "Current time: 2025-03-26 14:39:57. Total running time: 5min 30s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status        lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00000   RUNNING    0.1               16        8            304.856   3.27373    0.0372196 |\n",
      "| train_emnist_6b36b_00001   PENDING    0.1               32                                                    |\n",
      "| train_emnist_6b36b_00002   PENDING    0.1               64                                                    |\n",
      "| train_emnist_6b36b_00003   PENDING    0.01              16                                                    |\n",
      "| train_emnist_6b36b_00004   PENDING    0.01              32                                                    |\n",
      "| train_emnist_6b36b_00005   PENDING    0.01              64                                                    |\n",
      "| train_emnist_6b36b_00006   PENDING    0.001             16                                                    |\n",
      "| train_emnist_6b36b_00007   PENDING    0.001             32                                                    |\n",
      "| train_emnist_6b36b_00008   PENDING    0.001             64                                                    |\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=5148)\u001b[0m [9,  6000] loss: 1.091\n",
      "\n",
      "Trial train_emnist_6b36b_00000 finished iteration 9 at 2025-03-26 14:40:14. Total running time: 5min 48s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00000 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000008 |\n",
      "| time_this_iter_s                                    37.08438 |\n",
      "| time_total_s                                       341.94061 |\n",
      "| training_iteration                                         9 |\n",
      "| accuracy                                             0.03866 |\n",
      "| loss                                      3.2717504501342773 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00000 saved a checkpoint for iteration 9 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00000_0_batch_size=16,lr=0.1000_2025-03-26_14-34-26/checkpoint_000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=5148)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00000_0_batch_size=16,lr=0.1000_2025-03-26_14-34-26/checkpoint_000008)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=5148)\u001b[0m [10,  2000] loss: 3.273\n",
      "\n",
      "Trial status: 1 RUNNING | 8 PENDING\n",
      "Current time: 2025-03-26 14:40:27. Total running time: 6min 0s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status        lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00000   RUNNING    0.1               16        9            341.941   3.27175    0.0386619 |\n",
      "| train_emnist_6b36b_00001   PENDING    0.1               32                                                    |\n",
      "| train_emnist_6b36b_00002   PENDING    0.1               64                                                    |\n",
      "| train_emnist_6b36b_00003   PENDING    0.01              16                                                    |\n",
      "| train_emnist_6b36b_00004   PENDING    0.01              32                                                    |\n",
      "| train_emnist_6b36b_00005   PENDING    0.01              64                                                    |\n",
      "| train_emnist_6b36b_00006   PENDING    0.001             16                                                    |\n",
      "| train_emnist_6b36b_00007   PENDING    0.001             32                                                    |\n",
      "| train_emnist_6b36b_00008   PENDING    0.001             64                                                    |\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=5148)\u001b[0m [10,  4000] loss: 1.637\n",
      "\u001b[36m(func pid=5148)\u001b[0m [10,  6000] loss: 1.091\n",
      "\n",
      "Trial train_emnist_6b36b_00000 finished iteration 10 at 2025-03-26 14:40:51. Total running time: 6min 24s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00000 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000009 |\n",
      "| time_this_iter_s                                    36.64319 |\n",
      "| time_total_s                                        378.5838 |\n",
      "| training_iteration                                        10 |\n",
      "| accuracy                                             0.03698 |\n",
      "| loss                                      3.2788407802581787 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00000 saved a checkpoint for iteration 10 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00000_0_batch_size=16,lr=0.1000_2025-03-26_14-34-26/checkpoint_000009\n",
      "\n",
      "Trial train_emnist_6b36b_00000 completed after 10 iterations at 2025-03-26 14:40:51. Total running time: 6min 24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=5148)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00000_0_batch_size=16,lr=0.1000_2025-03-26_14-34-26/checkpoint_000009)\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb: uploading history steps 8-9, summary\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb:                                                                                \n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb: Run history:\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb:                 accuracy ‚ñÅ‚ñÅ‚ñÖ‚ñÅ‚ñÇ‚ñÑ‚ñà‚ñÇ‚ñÑ‚ñÅ\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb: iterations_since_restore ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb:                     loss ‚ñÖ‚ñÉ‚ñÑ‚ñÅ‚ñà‚ñÉ‚ñÅ‚ñÖ‚ñÑ‚ñá\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb:       time_since_restore ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb:         time_this_iter_s ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb:             time_total_s ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb:                timestamp ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb:       training_iteration ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb: Run summary:\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb:                 accuracy 0.03698\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb: iterations_since_restore 10\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb:                     loss 3.27884\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb:       time_since_restore 378.5838\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb:         time_this_iter_s 36.64319\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb:             time_total_s 378.5838\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb:                timestamp 1743000051\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb:       training_iteration 10\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb: üöÄ View run train_emnist_6b36b_00000 at: https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2/runs/6b36b_00000\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb: ‚≠êÔ∏è View project at: https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[36m(_WandbLoggingActor pid=5213)\u001b[0m wandb: Find logs at: ./wandb/run-20250326_143435-6b36b_00000/logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 1 TERMINATED | 8 PENDING\n",
      "Current time: 2025-03-26 14:40:57. Total running time: 6min 30s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884    0.0369792 |\n",
      "| train_emnist_6b36b_00001   PENDING      0.1               32                                                    |\n",
      "| train_emnist_6b36b_00002   PENDING      0.1               64                                                    |\n",
      "| train_emnist_6b36b_00003   PENDING      0.01              16                                                    |\n",
      "| train_emnist_6b36b_00004   PENDING      0.01              32                                                    |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                    |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                    |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                    |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                    |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_emnist_6b36b_00001 started with configuration:\n",
      "+-----------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00001 config         |\n",
      "+-----------------------------------------------+\n",
      "| batch_size                                 32 |\n",
      "| lr                                        0.1 |\n",
      "+-----------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb: Currently logged in as: shadaevf (shadaevf-rtu-mirea) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb: Tracking run with wandb version 0.19.8\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2025-03-26_14-34-21_606425_267/artifacts/2025-03-26_14-34-26/train_emnist_2025-03-26_14-34-26/driver_artifacts/train_emnist_6b36b_00001_1_batch_size=32,lr=0.1000_2025-03-26_14-34-26/wandb/run-20250326_144102-6b36b_00001\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb: Syncing run train_emnist_6b36b_00001\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb: ‚≠êÔ∏è View project at https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb: üöÄ View run at https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2/runs/6b36b_00001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=7136)\u001b[0m [1,  2000] loss: 3.107\n",
      "\n",
      "Trial status: 1 TERMINATED | 1 RUNNING | 7 PENDING\n",
      "Current time: 2025-03-26 14:41:27. Total running time: 7min 0s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00001   RUNNING      0.1               32                                                    |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884    0.0369792 |\n",
      "| train_emnist_6b36b_00002   PENDING      0.1               64                                                    |\n",
      "| train_emnist_6b36b_00003   PENDING      0.01              16                                                    |\n",
      "| train_emnist_6b36b_00004   PENDING      0.01              32                                                    |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                    |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                    |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                    |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                    |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=7136)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00001_1_batch_size=32,lr=0.1000_2025-03-26_14-34-26/checkpoint_000000)\n",
      "2025-03-26 14:41:30,334\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.224 s, which may be a performance bottleneck.\n",
      "2025-03-26 14:41:30,337\tWARNING util.py:201 -- The `process_trial_result` operation took 1.234 s, which may be a performance bottleneck.\n",
      "2025-03-26 14:41:30,339\tWARNING util.py:201 -- Processing trial results took 1.236 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-03-26 14:41:30,340\tWARNING util.py:201 -- The `process_trial_result` operation took 1.238 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial train_emnist_6b36b_00001 finished iteration 1 at 2025-03-26 14:41:30. Total running time: 7min 3s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00001 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000000 |\n",
      "| time_this_iter_s                                    30.99733 |\n",
      "| time_total_s                                        30.99733 |\n",
      "| training_iteration                                         1 |\n",
      "| accuracy                                             0.09443 |\n",
      "| loss                                      3.1123929023742676 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00001 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00001_1_batch_size=32,lr=0.1000_2025-03-26_14-34-26/checkpoint_000000\n",
      "\u001b[36m(func pid=7136)\u001b[0m [2,  2000] loss: 3.201\n",
      "\n",
      "Trial status: 1 TERMINATED | 1 RUNNING | 7 PENDING\n",
      "Current time: 2025-03-26 14:41:57. Total running time: 7min 30s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00001   RUNNING      0.1               32        1            30.9973   3.11239    0.0944311 |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10           378.584    3.27884    0.0369792 |\n",
      "| train_emnist_6b36b_00002   PENDING      0.1               64                                                    |\n",
      "| train_emnist_6b36b_00003   PENDING      0.01              16                                                    |\n",
      "| train_emnist_6b36b_00004   PENDING      0.01              32                                                    |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                    |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                    |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                    |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                    |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_emnist_6b36b_00001 finished iteration 2 at 2025-03-26 14:41:58. Total running time: 7min 32s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00001 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000001 |\n",
      "| time_this_iter_s                                    28.31591 |\n",
      "| time_total_s                                        59.31324 |\n",
      "| training_iteration                                         2 |\n",
      "| accuracy                                             0.03858 |\n",
      "| loss                                      3.2622640132904053 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00001 saved a checkpoint for iteration 2 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00001_1_batch_size=32,lr=0.1000_2025-03-26_14-34-26/checkpoint_000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=7136)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00001_1_batch_size=32,lr=0.1000_2025-03-26_14-34-26/checkpoint_000001)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=7136)\u001b[0m [3,  2000] loss: 3.267\n",
      "\n",
      "Trial train_emnist_6b36b_00001 finished iteration 3 at 2025-03-26 14:42:27. Total running time: 8min 0s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00001 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000002 |\n",
      "| time_this_iter_s                                    28.59928 |\n",
      "| time_total_s                                        87.91253 |\n",
      "| training_iteration                                         3 |\n",
      "| accuracy                                             0.03866 |\n",
      "| loss                                      3.2628161907196045 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00001 saved a checkpoint for iteration 3 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00001_1_batch_size=32,lr=0.1000_2025-03-26_14-34-26/checkpoint_000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=7136)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00001_1_batch_size=32,lr=0.1000_2025-03-26_14-34-26/checkpoint_000002)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 1 TERMINATED | 1 RUNNING | 7 PENDING\n",
      "Current time: 2025-03-26 14:42:27. Total running time: 8min 1s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00001   RUNNING      0.1               32        3            87.9125   3.26282    0.0386619 |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10           378.584    3.27884    0.0369792 |\n",
      "| train_emnist_6b36b_00002   PENDING      0.1               64                                                    |\n",
      "| train_emnist_6b36b_00003   PENDING      0.01              16                                                    |\n",
      "| train_emnist_6b36b_00004   PENDING      0.01              32                                                    |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                    |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                    |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                    |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                    |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=7136)\u001b[0m [4,  2000] loss: 3.266\n",
      "\n",
      "Trial train_emnist_6b36b_00001 finished iteration 4 at 2025-03-26 14:42:55. Total running time: 8min 28s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00001 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000003 |\n",
      "| time_this_iter_s                                     28.2483 |\n",
      "| time_total_s                                       116.16082 |\n",
      "| training_iteration                                         4 |\n",
      "| accuracy                                             0.03842 |\n",
      "| loss                                      3.2679367065429688 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00001 saved a checkpoint for iteration 4 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00001_1_batch_size=32,lr=0.1000_2025-03-26_14-34-26/checkpoint_000003\n",
      "\n",
      "Trial train_emnist_6b36b_00001 completed after 4 iterations at 2025-03-26 14:42:55. Total running time: 8min 28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=7136)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00001_1_batch_size=32,lr=0.1000_2025-03-26_14-34-26/checkpoint_000003)\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb: uploading history steps 2-3, summary\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb:                                                                                \n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb: Run history:\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb:                 accuracy ‚ñà‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb: iterations_since_restore ‚ñÅ‚ñÉ‚ñÜ‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb:                     loss ‚ñÅ‚ñà‚ñà‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb:       time_since_restore ‚ñÅ‚ñÉ‚ñÜ‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb:         time_this_iter_s ‚ñà‚ñÅ‚ñÇ‚ñÅ\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb:             time_total_s ‚ñÅ‚ñÉ‚ñÜ‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb:                timestamp ‚ñÅ‚ñÅ‚ñà‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb:       training_iteration ‚ñÅ‚ñÉ‚ñÜ‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb: Run summary:\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb:                 accuracy 0.03842\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb:                     loss 3.26794\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb:       time_since_restore 116.16082\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb:         time_this_iter_s 28.2483\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb:             time_total_s 116.16082\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb:                timestamp 1743000175\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb: üöÄ View run train_emnist_6b36b_00001 at: https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2/runs/6b36b_00001\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb: ‚≠êÔ∏è View project at: https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[36m(_WandbLoggingActor pid=7201)\u001b[0m wandb: Find logs at: ./wandb/run-20250326_144102-6b36b_00001/logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 2 TERMINATED | 7 PENDING\n",
      "Current time: 2025-03-26 14:42:57. Total running time: 8min 31s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884    0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794    0.0384215 |\n",
      "| train_emnist_6b36b_00002   PENDING      0.1               64                                                    |\n",
      "| train_emnist_6b36b_00003   PENDING      0.01              16                                                    |\n",
      "| train_emnist_6b36b_00004   PENDING      0.01              32                                                    |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                    |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                    |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                    |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                    |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_emnist_6b36b_00002 started with configuration:\n",
      "+-----------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00002 config         |\n",
      "+-----------------------------------------------+\n",
      "| batch_size                                 64 |\n",
      "| lr                                        0.1 |\n",
      "+-----------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb: Currently logged in as: shadaevf (shadaevf-rtu-mirea) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb: Tracking run with wandb version 0.19.8\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2025-03-26_14-34-21_606425_267/artifacts/2025-03-26_14-34-26/train_emnist_2025-03-26_14-34-26/driver_artifacts/train_emnist_6b36b_00002_2_batch_size=64,lr=0.1000_2025-03-26_14-34-26/wandb/run-20250326_144306-6b36b_00002\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb: Syncing run train_emnist_6b36b_00002\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb: ‚≠êÔ∏è View project at https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb: üöÄ View run at https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2/runs/6b36b_00002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 2 TERMINATED | 1 RUNNING | 6 PENDING\n",
      "Current time: 2025-03-26 14:43:27. Total running time: 9min 1s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00002   RUNNING      0.1               64                                                    |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884    0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794    0.0384215 |\n",
      "| train_emnist_6b36b_00003   PENDING      0.01              16                                                    |\n",
      "| train_emnist_6b36b_00004   PENDING      0.01              32                                                    |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                    |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                    |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                    |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                    |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=7903)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00002_2_batch_size=64,lr=0.1000_2025-03-26_14-34-26/checkpoint_000000)\n",
      "2025-03-26 14:43:29,632\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.237 s, which may be a performance bottleneck.\n",
      "2025-03-26 14:43:29,634\tWARNING util.py:201 -- The `process_trial_result` operation took 1.239 s, which may be a performance bottleneck.\n",
      "2025-03-26 14:43:29,636\tWARNING util.py:201 -- Processing trial results took 1.241 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-03-26 14:43:29,639\tWARNING util.py:201 -- The `process_trial_result` operation took 1.244 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial train_emnist_6b36b_00002 finished iteration 1 at 2025-03-26 14:43:29. Total running time: 9min 3s\n",
      "+-------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00002 result                       |\n",
      "+-------------------------------------------------------------+\n",
      "| checkpoint_dir_name                       checkpoint_000000 |\n",
      "| time_this_iter_s                                    27.5972 |\n",
      "| time_total_s                                        27.5972 |\n",
      "| training_iteration                                        1 |\n",
      "| accuracy                                            0.31763 |\n",
      "| loss                                      2.337292432785034 |\n",
      "+-------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00002 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00002_2_batch_size=64,lr=0.1000_2025-03-26_14-34-26/checkpoint_000000\n",
      "\n",
      "Trial train_emnist_6b36b_00002 finished iteration 2 at 2025-03-26 14:43:54. Total running time: 9min 27s\n",
      "+-------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00002 result                       |\n",
      "+-------------------------------------------------------------+\n",
      "| checkpoint_dir_name                       checkpoint_000001 |\n",
      "| time_this_iter_s                                   24.84441 |\n",
      "| time_total_s                                       52.44161 |\n",
      "| training_iteration                                        2 |\n",
      "| accuracy                                            0.31478 |\n",
      "| loss                                      2.409929037094116 |\n",
      "+-------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00002 saved a checkpoint for iteration 2 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00002_2_batch_size=64,lr=0.1000_2025-03-26_14-34-26/checkpoint_000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=7903)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00002_2_batch_size=64,lr=0.1000_2025-03-26_14-34-26/checkpoint_000001)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 2 TERMINATED | 1 RUNNING | 6 PENDING\n",
      "Current time: 2025-03-26 14:43:57. Total running time: 9min 31s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00002   RUNNING      0.1               64        2            52.4416   2.40993    0.314784  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10           378.584    3.27884    0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4           116.161    3.26794    0.0384215 |\n",
      "| train_emnist_6b36b_00003   PENDING      0.01              16                                                    |\n",
      "| train_emnist_6b36b_00004   PENDING      0.01              32                                                    |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                    |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                    |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                    |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                    |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_emnist_6b36b_00002 finished iteration 3 at 2025-03-26 14:44:18. Total running time: 9min 52s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00002 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000002 |\n",
      "| time_this_iter_s                                    24.28765 |\n",
      "| time_total_s                                        76.72926 |\n",
      "| training_iteration                                         3 |\n",
      "| accuracy                                             0.26795 |\n",
      "| loss                                      2.4767215251922607 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00002 saved a checkpoint for iteration 3 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00002_2_batch_size=64,lr=0.1000_2025-03-26_14-34-26/checkpoint_000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=7903)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00002_2_batch_size=64,lr=0.1000_2025-03-26_14-34-26/checkpoint_000002)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 2 TERMINATED | 1 RUNNING | 6 PENDING\n",
      "Current time: 2025-03-26 14:44:27. Total running time: 10min 1s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00002   RUNNING      0.1               64        3            76.7293   2.47672    0.267949  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10           378.584    3.27884    0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4           116.161    3.26794    0.0384215 |\n",
      "| train_emnist_6b36b_00003   PENDING      0.01              16                                                    |\n",
      "| train_emnist_6b36b_00004   PENDING      0.01              32                                                    |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                    |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                    |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                    |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                    |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_emnist_6b36b_00002 finished iteration 4 at 2025-03-26 14:44:42. Total running time: 10min 15s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00002 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000003 |\n",
      "| time_this_iter_s                                    23.66457 |\n",
      "| time_total_s                                       100.39382 |\n",
      "| training_iteration                                         4 |\n",
      "| accuracy                                             0.27925 |\n",
      "| loss                                      2.4716148376464844 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00002 saved a checkpoint for iteration 4 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00002_2_batch_size=64,lr=0.1000_2025-03-26_14-34-26/checkpoint_000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=7903)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00002_2_batch_size=64,lr=0.1000_2025-03-26_14-34-26/checkpoint_000003)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 2 TERMINATED | 1 RUNNING | 6 PENDING\n",
      "Current time: 2025-03-26 14:44:58. Total running time: 10min 31s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00002   RUNNING      0.1               64        4            100.394   2.47161    0.279247  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884    0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794    0.0384215 |\n",
      "| train_emnist_6b36b_00003   PENDING      0.01              16                                                    |\n",
      "| train_emnist_6b36b_00004   PENDING      0.01              32                                                    |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                    |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                    |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                    |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                    |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_emnist_6b36b_00002 finished iteration 5 at 2025-03-26 14:45:07. Total running time: 10min 40s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00002 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000004 |\n",
      "| time_this_iter_s                                    24.82854 |\n",
      "| time_total_s                                       125.22236 |\n",
      "| training_iteration                                         5 |\n",
      "| accuracy                                             0.24916 |\n",
      "| loss                                      2.5448684692382812 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00002 saved a checkpoint for iteration 5 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00002_2_batch_size=64,lr=0.1000_2025-03-26_14-34-26/checkpoint_000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=7903)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00002_2_batch_size=64,lr=0.1000_2025-03-26_14-34-26/checkpoint_000004)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 2 TERMINATED | 1 RUNNING | 6 PENDING\n",
      "Current time: 2025-03-26 14:45:28. Total running time: 11min 1s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00002   RUNNING      0.1               64        5            125.222   2.54487    0.249159  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884    0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794    0.0384215 |\n",
      "| train_emnist_6b36b_00003   PENDING      0.01              16                                                    |\n",
      "| train_emnist_6b36b_00004   PENDING      0.01              32                                                    |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                    |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                    |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                    |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                    |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_emnist_6b36b_00002 finished iteration 6 at 2025-03-26 14:45:32. Total running time: 11min 5s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00002 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000005 |\n",
      "| time_this_iter_s                                    24.98089 |\n",
      "| time_total_s                                       150.20326 |\n",
      "| training_iteration                                         6 |\n",
      "| accuracy                                             0.18786 |\n",
      "| loss                                      2.6988441944122314 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00002 saved a checkpoint for iteration 6 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00002_2_batch_size=64,lr=0.1000_2025-03-26_14-34-26/checkpoint_000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=7903)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00002_2_batch_size=64,lr=0.1000_2025-03-26_14-34-26/checkpoint_000005)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial train_emnist_6b36b_00002 finished iteration 7 at 2025-03-26 14:45:55. Total running time: 11min 29s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00002 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000006 |\n",
      "| time_this_iter_s                                    23.45067 |\n",
      "| time_total_s                                       173.65393 |\n",
      "| training_iteration                                         7 |\n",
      "| accuracy                                             0.16615 |\n",
      "| loss                                      2.8431167602539062 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00002 saved a checkpoint for iteration 7 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00002_2_batch_size=64,lr=0.1000_2025-03-26_14-34-26/checkpoint_000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=7903)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00002_2_batch_size=64,lr=0.1000_2025-03-26_14-34-26/checkpoint_000006)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 2 TERMINATED | 1 RUNNING | 6 PENDING\n",
      "Current time: 2025-03-26 14:45:58. Total running time: 11min 31s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00002   RUNNING      0.1               64        7            173.654   2.84312    0.166146  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884    0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794    0.0384215 |\n",
      "| train_emnist_6b36b_00003   PENDING      0.01              16                                                    |\n",
      "| train_emnist_6b36b_00004   PENDING      0.01              32                                                    |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                    |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                    |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                    |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                    |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_emnist_6b36b_00002 finished iteration 8 at 2025-03-26 14:46:20. Total running time: 11min 53s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00002 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000007 |\n",
      "| time_this_iter_s                                    24.75221 |\n",
      "| time_total_s                                       198.40614 |\n",
      "| training_iteration                                         8 |\n",
      "| accuracy                                             0.18389 |\n",
      "| loss                                      2.7372379302978516 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00002 saved a checkpoint for iteration 8 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00002_2_batch_size=64,lr=0.1000_2025-03-26_14-34-26/checkpoint_000007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=7903)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00002_2_batch_size=64,lr=0.1000_2025-03-26_14-34-26/checkpoint_000007)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 2 TERMINATED | 1 RUNNING | 6 PENDING\n",
      "Current time: 2025-03-26 14:46:28. Total running time: 12min 1s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00002   RUNNING      0.1               64        8            198.406   2.73724    0.183894  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884    0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794    0.0384215 |\n",
      "| train_emnist_6b36b_00003   PENDING      0.01              16                                                    |\n",
      "| train_emnist_6b36b_00004   PENDING      0.01              32                                                    |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                    |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                    |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                    |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                    |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=7903)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00002_2_batch_size=64,lr=0.1000_2025-03-26_14-34-26/checkpoint_000008)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial train_emnist_6b36b_00002 finished iteration 9 at 2025-03-26 14:46:45. Total running time: 12min 18s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00002 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000008 |\n",
      "| time_this_iter_s                                    25.04269 |\n",
      "| time_total_s                                       223.44883 |\n",
      "| training_iteration                                         9 |\n",
      "| accuracy                                             0.12584 |\n",
      "| loss                                      2.9635722637176514 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00002 saved a checkpoint for iteration 9 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00002_2_batch_size=64,lr=0.1000_2025-03-26_14-34-26/checkpoint_000008\n",
      "\n",
      "Trial status: 2 TERMINATED | 1 RUNNING | 6 PENDING\n",
      "Current time: 2025-03-26 14:46:58. Total running time: 12min 31s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00002   RUNNING      0.1               64        9            223.449   2.96357    0.125841  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884    0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794    0.0384215 |\n",
      "| train_emnist_6b36b_00003   PENDING      0.01              16                                                    |\n",
      "| train_emnist_6b36b_00004   PENDING      0.01              32                                                    |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                    |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                    |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                    |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                    |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_emnist_6b36b_00002 finished iteration 10 at 2025-03-26 14:47:09. Total running time: 12min 42s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00002 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000009 |\n",
      "| time_this_iter_s                                    23.44257 |\n",
      "| time_total_s                                       246.89139 |\n",
      "| training_iteration                                        10 |\n",
      "| accuracy                                             0.16506 |\n",
      "| loss                                      2.8079564571380615 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00002 saved a checkpoint for iteration 10 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00002_2_batch_size=64,lr=0.1000_2025-03-26_14-34-26/checkpoint_000009\n",
      "\n",
      "Trial train_emnist_6b36b_00002 completed after 10 iterations at 2025-03-26 14:47:09. Total running time: 12min 42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=7903)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00002_2_batch_size=64,lr=0.1000_2025-03-26_14-34-26/checkpoint_000009)\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb: uploading history steps 8-9, summary\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb:                                                                                \n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb: Run history:\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb:                 accuracy ‚ñà‚ñà‚ñÜ‚ñá‚ñÜ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb: iterations_since_restore ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb:                     loss ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñá‚ñÖ‚ñà‚ñÜ\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb:       time_since_restore ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb:         time_this_iter_s ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÅ\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb:             time_total_s ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb:                timestamp ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb:       training_iteration ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb: Run summary:\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb:                 accuracy 0.16506\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb: iterations_since_restore 10\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb:                     loss 2.80796\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb:       time_since_restore 246.89139\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb:         time_this_iter_s 23.44257\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb:             time_total_s 246.89139\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb:                timestamp 1743000429\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb:       training_iteration 10\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb: üöÄ View run train_emnist_6b36b_00002 at: https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2/runs/6b36b_00002\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb: ‚≠êÔ∏è View project at: https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[36m(_WandbLoggingActor pid=7958)\u001b[0m wandb: Find logs at: ./wandb/run-20250326_144306-6b36b_00002/logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial train_emnist_6b36b_00003 started with configuration:\n",
      "+------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00003 config          |\n",
      "+------------------------------------------------+\n",
      "| batch_size                                  16 |\n",
      "| lr                                        0.01 |\n",
      "+------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb: Currently logged in as: shadaevf (shadaevf-rtu-mirea) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb: Tracking run with wandb version 0.19.8\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2025-03-26_14-34-21_606425_267/artifacts/2025-03-26_14-34-26/train_emnist_2025-03-26_14-34-26/driver_artifacts/train_emnist_6b36b_00003_3_batch_size=16,lr=0.0100_2025-03-26_14-34-26/wandb/run-20250326_144719-6b36b_00003\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb: Syncing run train_emnist_6b36b_00003\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb: ‚≠êÔ∏è View project at https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb: üöÄ View run at https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2/runs/6b36b_00003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 3 TERMINATED | 1 RUNNING | 5 PENDING\n",
      "Current time: 2025-03-26 14:47:28. Total running time: 13min 1s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00003   RUNNING      0.01              16                                                    |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884    0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794    0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796    0.165064  |\n",
      "| train_emnist_6b36b_00004   PENDING      0.01              32                                                    |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                    |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                    |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                    |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                    |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=9323)\u001b[0m [1,  2000] loss: 1.098\n",
      "\u001b[36m(func pid=9323)\u001b[0m [1,  4000] loss: 0.371\n",
      "\u001b[36m(func pid=9323)\u001b[0m [1,  6000] loss: 0.219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=9323)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00003_3_batch_size=16,lr=0.0100_2025-03-26_14-34-26/checkpoint_000000)\n",
      "2025-03-26 14:47:59,555\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.241 s, which may be a performance bottleneck.\n",
      "2025-03-26 14:47:59,557\tWARNING util.py:201 -- The `process_trial_result` operation took 1.244 s, which may be a performance bottleneck.\n",
      "2025-03-26 14:47:59,559\tWARNING util.py:201 -- Processing trial results took 1.246 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-03-26 14:47:59,559\tWARNING util.py:201 -- The `process_trial_result` operation took 1.247 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial train_emnist_6b36b_00003 finished iteration 1 at 2025-03-26 14:47:59. Total running time: 13min 32s\n",
      "+-------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00003 result                       |\n",
      "+-------------------------------------------------------------+\n",
      "| checkpoint_dir_name                       checkpoint_000000 |\n",
      "| time_this_iter_s                                   42.39473 |\n",
      "| time_total_s                                       42.39473 |\n",
      "| training_iteration                                        1 |\n",
      "| accuracy                                            0.81635 |\n",
      "| loss                                      0.610586404800415 |\n",
      "+-------------------------------------------------------------+\n",
      "\n",
      "Trial status: 3 TERMINATED | 1 RUNNING | 5 PENDING\n",
      "Current time: 2025-03-26 14:47:59. Total running time: 13min 32s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00003   RUNNING      0.01              16        1            42.3947   0.610586    0.816346  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10           378.584    3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4           116.161    3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10           246.891    2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00004   PENDING      0.01              32                                                     |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                     |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_emnist_6b36b_00003 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00003_3_batch_size=16,lr=0.0100_2025-03-26_14-34-26/checkpoint_000000\n",
      "\u001b[36m(func pid=9323)\u001b[0m [2,  2000] loss: 0.583\n",
      "\u001b[36m(func pid=9323)\u001b[0m [2,  4000] loss: 0.277\n",
      "\u001b[36m(func pid=9323)\u001b[0m [2,  6000] loss: 0.186\n",
      "\n",
      "Trial status: 3 TERMINATED | 1 RUNNING | 5 PENDING\n",
      "Current time: 2025-03-26 14:48:29. Total running time: 14min 2s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00003   RUNNING      0.01              16        1            42.3947   0.610586    0.816346  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10           378.584    3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4           116.161    3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10           246.891    2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00004   PENDING      0.01              32                                                     |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                     |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_emnist_6b36b_00003 finished iteration 2 at 2025-03-26 14:48:37. Total running time: 14min 10s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00003 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000001 |\n",
      "| time_this_iter_s                                    37.74494 |\n",
      "| time_total_s                                        80.13966 |\n",
      "| training_iteration                                         2 |\n",
      "| accuracy                                             0.83554 |\n",
      "| loss                                      0.5568307042121887 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00003 saved a checkpoint for iteration 2 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00003_3_batch_size=16,lr=0.0100_2025-03-26_14-34-26/checkpoint_000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=9323)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00003_3_batch_size=16,lr=0.0100_2025-03-26_14-34-26/checkpoint_000001)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=9323)\u001b[0m [3,  2000] loss: 0.509\n",
      "\u001b[36m(func pid=9323)\u001b[0m [3,  4000] loss: 0.251\n",
      "\n",
      "Trial status: 3 TERMINATED | 1 RUNNING | 5 PENDING\n",
      "Current time: 2025-03-26 14:48:59. Total running time: 14min 32s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00003   RUNNING      0.01              16        2            80.1397   0.556831    0.835537  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10           378.584    3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4           116.161    3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10           246.891    2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00004   PENDING      0.01              32                                                     |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                     |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=9323)\u001b[0m [3,  6000] loss: 0.170\n",
      "\n",
      "Trial train_emnist_6b36b_00003 finished iteration 3 at 2025-03-26 14:49:15. Total running time: 14min 48s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00003 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000002 |\n",
      "| time_this_iter_s                                    38.04515 |\n",
      "| time_total_s                                       118.18481 |\n",
      "| training_iteration                                         3 |\n",
      "| accuracy                                             0.84151 |\n",
      "| loss                                      0.5410831570625305 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00003 saved a checkpoint for iteration 3 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00003_3_batch_size=16,lr=0.0100_2025-03-26_14-34-26/checkpoint_000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=9323)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00003_3_batch_size=16,lr=0.0100_2025-03-26_14-34-26/checkpoint_000002)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=9323)\u001b[0m [4,  2000] loss: 0.462\n",
      "\n",
      "Trial status: 3 TERMINATED | 1 RUNNING | 5 PENDING\n",
      "Current time: 2025-03-26 14:49:29. Total running time: 15min 3s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00003   RUNNING      0.01              16        3            118.185   0.541083    0.841506  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00004   PENDING      0.01              32                                                     |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                     |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=9323)\u001b[0m [4,  4000] loss: 0.238\n",
      "\u001b[36m(func pid=9323)\u001b[0m [4,  6000] loss: 0.157\n",
      "\n",
      "Trial train_emnist_6b36b_00003 finished iteration 4 at 2025-03-26 14:49:54. Total running time: 15min 27s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00003 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000003 |\n",
      "| time_this_iter_s                                    38.78614 |\n",
      "| time_total_s                                       156.97095 |\n",
      "| training_iteration                                         4 |\n",
      "| accuracy                                             0.85052 |\n",
      "| loss                                      0.5105500221252441 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00003 saved a checkpoint for iteration 4 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00003_3_batch_size=16,lr=0.0100_2025-03-26_14-34-26/checkpoint_000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=9323)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00003_3_batch_size=16,lr=0.0100_2025-03-26_14-34-26/checkpoint_000003)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 3 TERMINATED | 1 RUNNING | 5 PENDING\n",
      "Current time: 2025-03-26 14:49:59. Total running time: 15min 33s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00003   RUNNING      0.01              16        4            156.971   0.51055    0.850521  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884    0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794    0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796    0.165064  |\n",
      "| train_emnist_6b36b_00004   PENDING      0.01              32                                                    |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                    |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                    |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                    |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                    |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=9323)\u001b[0m [5,  2000] loss: 0.432\n",
      "\u001b[36m(func pid=9323)\u001b[0m [5,  4000] loss: 0.215\n",
      "\u001b[36m(func pid=9323)\u001b[0m [5,  6000] loss: 0.150\n",
      "Trial status: 3 TERMINATED | 1 RUNNING | 5 PENDING\n",
      "Current time: 2025-03-26 14:50:29. Total running time: 16min 3s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)      loss     accuracy |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00003   RUNNING      0.01              16        4            156.971   0.51055    0.850521  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884    0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794    0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796    0.165064  |\n",
      "| train_emnist_6b36b_00004   PENDING      0.01              32                                                    |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                    |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                    |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                    |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                    |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_emnist_6b36b_00003 finished iteration 5 at 2025-03-26 14:50:32. Total running time: 16min 6s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00003 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000004 |\n",
      "| time_this_iter_s                                    38.49264 |\n",
      "| time_total_s                                       195.46359 |\n",
      "| training_iteration                                         5 |\n",
      "| accuracy                                             0.85232 |\n",
      "| loss                                      0.5124924182891846 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00003 saved a checkpoint for iteration 5 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00003_3_batch_size=16,lr=0.0100_2025-03-26_14-34-26/checkpoint_000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=9323)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00003_3_batch_size=16,lr=0.0100_2025-03-26_14-34-26/checkpoint_000004)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=9323)\u001b[0m [6,  2000] loss: 0.402\n",
      "\u001b[36m(func pid=9323)\u001b[0m [6,  4000] loss: 0.210\n",
      "\n",
      "Trial status: 3 TERMINATED | 1 RUNNING | 5 PENDING\n",
      "Current time: 2025-03-26 14:50:59. Total running time: 16min 33s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00003   RUNNING      0.01              16        5            195.464   0.512492    0.852324  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00004   PENDING      0.01              32                                                     |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                     |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=9323)\u001b[0m [6,  6000] loss: 0.144\n",
      "\n",
      "Trial train_emnist_6b36b_00003 finished iteration 6 at 2025-03-26 14:51:09. Total running time: 16min 43s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00003 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000005 |\n",
      "| time_this_iter_s                                    37.07813 |\n",
      "| time_total_s                                       232.54173 |\n",
      "| training_iteration                                         6 |\n",
      "| accuracy                                             0.84976 |\n",
      "| loss                                      0.5390985608100891 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00003 saved a checkpoint for iteration 6 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00003_3_batch_size=16,lr=0.0100_2025-03-26_14-34-26/checkpoint_000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=9323)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00003_3_batch_size=16,lr=0.0100_2025-03-26_14-34-26/checkpoint_000005)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=9323)\u001b[0m [7,  2000] loss: 0.385\n",
      "\n",
      "Trial status: 3 TERMINATED | 1 RUNNING | 5 PENDING\n",
      "Current time: 2025-03-26 14:51:29. Total running time: 17min 3s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00003   RUNNING      0.01              16        6            232.542   0.539099    0.84976   |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00004   PENDING      0.01              32                                                     |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                     |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=9323)\u001b[0m [7,  4000] loss: 0.201\n",
      "\u001b[36m(func pid=9323)\u001b[0m [7,  6000] loss: 0.141\n",
      "\n",
      "Trial train_emnist_6b36b_00003 finished iteration 7 at 2025-03-26 14:51:49. Total running time: 17min 22s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00003 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000006 |\n",
      "| time_this_iter_s                                    39.48084 |\n",
      "| time_total_s                                       272.02257 |\n",
      "| training_iteration                                         7 |\n",
      "| accuracy                                             0.85665 |\n",
      "| loss                                      0.5157279968261719 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00003 saved a checkpoint for iteration 7 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00003_3_batch_size=16,lr=0.0100_2025-03-26_14-34-26/checkpoint_000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=9323)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00003_3_batch_size=16,lr=0.0100_2025-03-26_14-34-26/checkpoint_000006)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 3 TERMINATED | 1 RUNNING | 5 PENDING\n",
      "Current time: 2025-03-26 14:51:59. Total running time: 17min 33s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00003   RUNNING      0.01              16        7            272.023   0.515728    0.856651  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00004   PENDING      0.01              32                                                     |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                     |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=9323)\u001b[0m [8,  2000] loss: 0.378\n",
      "\u001b[36m(func pid=9323)\u001b[0m [8,  4000] loss: 0.192\n",
      "\u001b[36m(func pid=9323)\u001b[0m [8,  6000] loss: 0.134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=9323)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00003_3_batch_size=16,lr=0.0100_2025-03-26_14-34-26/checkpoint_000007)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial train_emnist_6b36b_00003 finished iteration 8 at 2025-03-26 14:52:28. Total running time: 18min 1s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00003 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000007 |\n",
      "| time_this_iter_s                                    39.09352 |\n",
      "| time_total_s                                       311.11609 |\n",
      "| training_iteration                                         8 |\n",
      "| accuracy                                             0.85853 |\n",
      "| loss                                      0.5210479497909546 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00003 saved a checkpoint for iteration 8 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00003_3_batch_size=16,lr=0.0100_2025-03-26_14-34-26/checkpoint_000007\n",
      "\n",
      "Trial status: 3 TERMINATED | 1 RUNNING | 5 PENDING\n",
      "Current time: 2025-03-26 14:52:29. Total running time: 18min 3s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00003   RUNNING      0.01              16        8            311.116   0.521048    0.858534  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00004   PENDING      0.01              32                                                     |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                     |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=9323)\u001b[0m [9,  2000] loss: 0.353\n",
      "\u001b[36m(func pid=9323)\u001b[0m [9,  4000] loss: 0.193\n",
      "\u001b[36m(func pid=9323)\u001b[0m [9,  6000] loss: 0.127\n",
      "Trial status: 3 TERMINATED | 1 RUNNING | 5 PENDING\n",
      "Current time: 2025-03-26 14:53:00. Total running time: 18min 33s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00003   RUNNING      0.01              16        8            311.116   0.521048    0.858534  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00004   PENDING      0.01              32                                                     |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                     |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_emnist_6b36b_00003 finished iteration 9 at 2025-03-26 14:53:05. Total running time: 18min 38s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00003 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000008 |\n",
      "| time_this_iter_s                                    37.03317 |\n",
      "| time_total_s                                       348.14926 |\n",
      "| training_iteration                                         9 |\n",
      "| accuracy                                             0.85925 |\n",
      "| loss                                      0.5172367095947266 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00003 saved a checkpoint for iteration 9 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00003_3_batch_size=16,lr=0.0100_2025-03-26_14-34-26/checkpoint_000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=9323)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00003_3_batch_size=16,lr=0.0100_2025-03-26_14-34-26/checkpoint_000008)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=9323)\u001b[0m [10,  2000] loss: 0.344\n",
      "\u001b[36m(func pid=9323)\u001b[0m [10,  4000] loss: 0.183\n",
      "\n",
      "Trial status: 3 TERMINATED | 1 RUNNING | 5 PENDING\n",
      "Current time: 2025-03-26 14:53:30. Total running time: 19min 3s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00003   RUNNING      0.01              16        9            348.149   0.517237    0.859255  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00004   PENDING      0.01              32                                                     |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                     |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=9323)\u001b[0m [10,  6000] loss: 0.129\n",
      "\n",
      "Trial train_emnist_6b36b_00003 finished iteration 10 at 2025-03-26 14:53:44. Total running time: 19min 17s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00003 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000009 |\n",
      "| time_this_iter_s                                     38.7165 |\n",
      "| time_total_s                                       386.86576 |\n",
      "| training_iteration                                        10 |\n",
      "| accuracy                                             0.85461 |\n",
      "| loss                                      0.5365285277366638 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00003 saved a checkpoint for iteration 10 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00003_3_batch_size=16,lr=0.0100_2025-03-26_14-34-26/checkpoint_000009\n",
      "\n",
      "Trial train_emnist_6b36b_00003 completed after 10 iterations at 2025-03-26 14:53:44. Total running time: 19min 17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=9323)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00003_3_batch_size=16,lr=0.0100_2025-03-26_14-34-26/checkpoint_000009)\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb: uploading config.yaml\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb: uploading history steps 8-9, summary\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb:                                                                                \n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb: Run history:\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb:                 accuracy ‚ñÅ‚ñÑ‚ñÖ‚ñá‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñá\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb: iterations_since_restore ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb:                     loss ‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÉ\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb:       time_since_restore ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb:         time_this_iter_s ‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÑ‚ñÑ‚ñÅ‚ñÉ\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb:             time_total_s ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb:                timestamp ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb:       training_iteration ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb: Run summary:\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb:                 accuracy 0.85461\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb: iterations_since_restore 10\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb:                     loss 0.53653\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb:       time_since_restore 386.86576\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb:         time_this_iter_s 38.7165\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb:             time_total_s 386.86576\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb:                timestamp 1743000824\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb:       training_iteration 10\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb: üöÄ View run train_emnist_6b36b_00003 at: https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2/runs/6b36b_00003\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb: ‚≠êÔ∏è View project at: https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[36m(_WandbLoggingActor pid=9380)\u001b[0m wandb: Find logs at: ./wandb/run-20250326_144719-6b36b_00003/logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial train_emnist_6b36b_00004 started with configuration:\n",
      "+------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00004 config          |\n",
      "+------------------------------------------------+\n",
      "| batch_size                                  32 |\n",
      "| lr                                        0.01 |\n",
      "+------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb: Currently logged in as: shadaevf (shadaevf-rtu-mirea) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb: Tracking run with wandb version 0.19.8\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2025-03-26_14-34-21_606425_267/artifacts/2025-03-26_14-34-26/train_emnist_2025-03-26_14-34-26/driver_artifacts/train_emnist_6b36b_00004_4_batch_size=32,lr=0.0100_2025-03-26_14-34-26/wandb/run-20250326_145355-6b36b_00004\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb: Syncing run train_emnist_6b36b_00004\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb: ‚≠êÔ∏è View project at https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb: üöÄ View run at https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2/runs/6b36b_00004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 4 TERMINATED | 1 RUNNING | 4 PENDING\n",
      "Current time: 2025-03-26 14:54:00. Total running time: 19min 33s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00004   RUNNING      0.01              32                                                     |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10            386.866   0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                     |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=11329)\u001b[0m [1,  2000] loss: 0.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=11329)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00004_4_batch_size=32,lr=0.0100_2025-03-26_14-34-26/checkpoint_000000)\n",
      "2025-03-26 14:54:23,995\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.225 s, which may be a performance bottleneck.\n",
      "2025-03-26 14:54:23,997\tWARNING util.py:201 -- The `process_trial_result` operation took 1.227 s, which may be a performance bottleneck.\n",
      "2025-03-26 14:54:23,998\tWARNING util.py:201 -- Processing trial results took 1.227 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-03-26 14:54:23,998\tWARNING util.py:201 -- The `process_trial_result` operation took 1.228 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial train_emnist_6b36b_00004 finished iteration 1 at 2025-03-26 14:54:23. Total running time: 19min 57s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00004 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000000 |\n",
      "| time_this_iter_s                                    32.05339 |\n",
      "| time_total_s                                        32.05339 |\n",
      "| training_iteration                                         1 |\n",
      "| accuracy                                             0.84006 |\n",
      "| loss                                      0.5152568817138672 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00004 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00004_4_batch_size=32,lr=0.0100_2025-03-26_14-34-26/checkpoint_000000\n",
      "\n",
      "Trial status: 4 TERMINATED | 1 RUNNING | 4 PENDING\n",
      "Current time: 2025-03-26 14:54:30. Total running time: 20min 3s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00004   RUNNING      0.01              32        1            32.0534   0.515257    0.840064  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10           378.584    3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4           116.161    3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10           246.891    2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10           386.866    0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                     |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=11329)\u001b[0m [2,  2000] loss: 0.474\n",
      "\n",
      "Trial train_emnist_6b36b_00004 finished iteration 2 at 2025-03-26 14:54:52. Total running time: 20min 26s\n",
      "+---------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00004 result                         |\n",
      "+---------------------------------------------------------------+\n",
      "| checkpoint_dir_name                         checkpoint_000001 |\n",
      "| time_this_iter_s                                     28.64984 |\n",
      "| time_total_s                                         60.70323 |\n",
      "| training_iteration                                          2 |\n",
      "| accuracy                                              0.86719 |\n",
      "| loss                                      0.42848441004753113 |\n",
      "+---------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00004 saved a checkpoint for iteration 2 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00004_4_batch_size=32,lr=0.0100_2025-03-26_14-34-26/checkpoint_000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=11329)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00004_4_batch_size=32,lr=0.0100_2025-03-26_14-34-26/checkpoint_000001)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 4 TERMINATED | 1 RUNNING | 4 PENDING\n",
      "Current time: 2025-03-26 14:55:00. Total running time: 20min 33s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00004   RUNNING      0.01              32        2            60.7032   0.428484    0.867188  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10           378.584    3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4           116.161    3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10           246.891    2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10           386.866    0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                     |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=11329)\u001b[0m [3,  2000] loss: 0.388\n",
      "\n",
      "Trial train_emnist_6b36b_00004 finished iteration 3 at 2025-03-26 14:55:21. Total running time: 20min 54s\n",
      "+---------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00004 result                         |\n",
      "+---------------------------------------------------------------+\n",
      "| checkpoint_dir_name                         checkpoint_000002 |\n",
      "| time_this_iter_s                                     28.77746 |\n",
      "| time_total_s                                         89.48069 |\n",
      "| training_iteration                                          3 |\n",
      "| accuracy                                              0.86506 |\n",
      "| loss                                      0.43447136878967285 |\n",
      "+---------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00004 saved a checkpoint for iteration 3 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00004_4_batch_size=32,lr=0.0100_2025-03-26_14-34-26/checkpoint_000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=11329)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00004_4_batch_size=32,lr=0.0100_2025-03-26_14-34-26/checkpoint_000002)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 4 TERMINATED | 1 RUNNING | 4 PENDING\n",
      "Current time: 2025-03-26 14:55:30. Total running time: 21min 3s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00004   RUNNING      0.01              32        3            89.4807   0.434471    0.865064  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10           378.584    3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4           116.161    3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10           246.891    2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10           386.866    0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                     |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=11329)\u001b[0m [4,  2000] loss: 0.350\n",
      "\n",
      "Trial train_emnist_6b36b_00004 finished iteration 4 at 2025-03-26 14:55:50. Total running time: 21min 23s\n",
      "+---------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00004 result                         |\n",
      "+---------------------------------------------------------------+\n",
      "| checkpoint_dir_name                         checkpoint_000003 |\n",
      "| time_this_iter_s                                       28.775 |\n",
      "| time_total_s                                        118.25569 |\n",
      "| training_iteration                                          4 |\n",
      "| accuracy                                              0.87644 |\n",
      "| loss                                      0.39543890953063965 |\n",
      "+---------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00004 saved a checkpoint for iteration 4 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00004_4_batch_size=32,lr=0.0100_2025-03-26_14-34-26/checkpoint_000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=11329)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00004_4_batch_size=32,lr=0.0100_2025-03-26_14-34-26/checkpoint_000003)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 4 TERMINATED | 1 RUNNING | 4 PENDING\n",
      "Current time: 2025-03-26 14:56:00. Total running time: 21min 33s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00004   RUNNING      0.01              32        4            118.256   0.395439    0.876442  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10            386.866   0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                     |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=11329)\u001b[0m [5,  2000] loss: 0.325\n",
      "\n",
      "Trial train_emnist_6b36b_00004 finished iteration 5 at 2025-03-26 14:56:18. Total running time: 21min 52s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00004 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000004 |\n",
      "| time_this_iter_s                                    28.75188 |\n",
      "| time_total_s                                       147.00757 |\n",
      "| training_iteration                                         5 |\n",
      "| accuracy                                             0.87981 |\n",
      "| loss                                      0.3864177465438843 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00004 saved a checkpoint for iteration 5 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00004_4_batch_size=32,lr=0.0100_2025-03-26_14-34-26/checkpoint_000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=11329)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00004_4_batch_size=32,lr=0.0100_2025-03-26_14-34-26/checkpoint_000004)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 4 TERMINATED | 1 RUNNING | 4 PENDING\n",
      "Current time: 2025-03-26 14:56:30. Total running time: 22min 3s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00004   RUNNING      0.01              32        5            147.008   0.386418    0.879808  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10            386.866   0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                     |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=11329)\u001b[0m [6,  2000] loss: 0.302\n",
      "\n",
      "Trial train_emnist_6b36b_00004 finished iteration 6 at 2025-03-26 14:56:47. Total running time: 22min 21s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00004 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000005 |\n",
      "| time_this_iter_s                                    28.81257 |\n",
      "| time_total_s                                       175.82014 |\n",
      "| training_iteration                                         6 |\n",
      "| accuracy                                             0.88462 |\n",
      "| loss                                      0.3797242343425751 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00004 saved a checkpoint for iteration 6 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00004_4_batch_size=32,lr=0.0100_2025-03-26_14-34-26/checkpoint_000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=11329)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00004_4_batch_size=32,lr=0.0100_2025-03-26_14-34-26/checkpoint_000005)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 4 TERMINATED | 1 RUNNING | 4 PENDING\n",
      "Current time: 2025-03-26 14:57:00. Total running time: 22min 33s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00004   RUNNING      0.01              32        6            175.82    0.379724    0.884615  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10            386.866   0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                     |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=11329)\u001b[0m [7,  2000] loss: 0.283\n",
      "\n",
      "Trial train_emnist_6b36b_00004 finished iteration 7 at 2025-03-26 14:57:16. Total running time: 22min 50s\n",
      "+---------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00004 result                         |\n",
      "+---------------------------------------------------------------+\n",
      "| checkpoint_dir_name                         checkpoint_000006 |\n",
      "| time_this_iter_s                                      28.8387 |\n",
      "| time_total_s                                        204.65884 |\n",
      "| training_iteration                                          7 |\n",
      "| accuracy                                              0.88429 |\n",
      "| loss                                      0.38823947310447693 |\n",
      "+---------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00004 saved a checkpoint for iteration 7 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00004_4_batch_size=32,lr=0.0100_2025-03-26_14-34-26/checkpoint_000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=11329)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00004_4_batch_size=32,lr=0.0100_2025-03-26_14-34-26/checkpoint_000006)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 4 TERMINATED | 1 RUNNING | 4 PENDING\n",
      "Current time: 2025-03-26 14:57:30. Total running time: 23min 3s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00004   RUNNING      0.01              32        7            204.659   0.388239    0.884295  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10            386.866   0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                     |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=11329)\u001b[0m [8,  2000] loss: 0.273\n",
      "\n",
      "Trial train_emnist_6b36b_00004 finished iteration 8 at 2025-03-26 14:57:45. Total running time: 23min 18s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00004 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000007 |\n",
      "| time_this_iter_s                                    28.78406 |\n",
      "| time_total_s                                       233.44291 |\n",
      "| training_iteration                                         8 |\n",
      "| accuracy                                             0.89151 |\n",
      "| loss                                      0.3661089837551117 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00004 saved a checkpoint for iteration 8 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00004_4_batch_size=32,lr=0.0100_2025-03-26_14-34-26/checkpoint_000007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=11329)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00004_4_batch_size=32,lr=0.0100_2025-03-26_14-34-26/checkpoint_000007)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=11329)\u001b[0m [9,  2000] loss: 0.257\n",
      "\n",
      "Trial status: 4 TERMINATED | 1 RUNNING | 4 PENDING\n",
      "Current time: 2025-03-26 14:58:00. Total running time: 23min 34s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00004   RUNNING      0.01              32        8            233.443   0.366109    0.891506  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10            386.866   0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                     |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_emnist_6b36b_00004 finished iteration 9 at 2025-03-26 14:58:14. Total running time: 23min 47s\n",
      "+---------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00004 result                         |\n",
      "+---------------------------------------------------------------+\n",
      "| checkpoint_dir_name                         checkpoint_000008 |\n",
      "| time_this_iter_s                                     28.61486 |\n",
      "| time_total_s                                        262.05777 |\n",
      "| training_iteration                                          9 |\n",
      "| accuracy                                               0.8877 |\n",
      "| loss                                      0.37637442350387573 |\n",
      "+---------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00004 saved a checkpoint for iteration 9 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00004_4_batch_size=32,lr=0.0100_2025-03-26_14-34-26/checkpoint_000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=11329)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00004_4_batch_size=32,lr=0.0100_2025-03-26_14-34-26/checkpoint_000008)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=11329)\u001b[0m [10,  2000] loss: 0.242\n",
      "\n",
      "Trial status: 4 TERMINATED | 1 RUNNING | 4 PENDING\n",
      "Current time: 2025-03-26 14:58:30. Total running time: 24min 4s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00004   RUNNING      0.01              32        9            262.058   0.376374    0.8877    |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10            386.866   0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00005   PENDING      0.01              64                                                     |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_emnist_6b36b_00004 finished iteration 10 at 2025-03-26 14:58:42. Total running time: 24min 15s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00004 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000009 |\n",
      "| time_this_iter_s                                    28.33207 |\n",
      "| time_total_s                                       290.38984 |\n",
      "| training_iteration                                        10 |\n",
      "| accuracy                                             0.88638 |\n",
      "| loss                                      0.3779756724834442 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00004 saved a checkpoint for iteration 10 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00004_4_batch_size=32,lr=0.0100_2025-03-26_14-34-26/checkpoint_000009\n",
      "\n",
      "Trial train_emnist_6b36b_00004 completed after 10 iterations at 2025-03-26 14:58:42. Total running time: 24min 15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=11329)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00004_4_batch_size=32,lr=0.0100_2025-03-26_14-34-26/checkpoint_000009)\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb: uploading history steps 8-9, summary\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb:                                                                                \n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb: Run history:\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb:                 accuracy ‚ñÅ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb: iterations_since_restore ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb:                     loss ‚ñà‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb:       time_since_restore ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb:         time_this_iter_s ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb:             time_total_s ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb:                timestamp ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb:       training_iteration ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb: Run summary:\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb:                 accuracy 0.88638\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb: iterations_since_restore 10\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb:                     loss 0.37798\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb:       time_since_restore 290.38984\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb:         time_this_iter_s 28.33207\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb:             time_total_s 290.38984\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb:                timestamp 1743001122\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb:       training_iteration 10\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb: üöÄ View run train_emnist_6b36b_00004 at: https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2/runs/6b36b_00004\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb: ‚≠êÔ∏è View project at: https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[36m(_WandbLoggingActor pid=11388)\u001b[0m wandb: Find logs at: ./wandb/run-20250326_145355-6b36b_00004/logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial train_emnist_6b36b_00005 started with configuration:\n",
      "+------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00005 config          |\n",
      "+------------------------------------------------+\n",
      "| batch_size                                  64 |\n",
      "| lr                                        0.01 |\n",
      "+------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb: Currently logged in as: shadaevf (shadaevf-rtu-mirea) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb: Tracking run with wandb version 0.19.8\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2025-03-26_14-34-21_606425_267/artifacts/2025-03-26_14-34-26/train_emnist_2025-03-26_14-34-26/driver_artifacts/train_emnist_6b36b_00005_5_batch_size=64,lr=0.0100_2025-03-26_14-34-26/wandb/run-20250326_145854-6b36b_00005\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb: Syncing run train_emnist_6b36b_00005\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb: ‚≠êÔ∏è View project at https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb: üöÄ View run at https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2/runs/6b36b_00005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 5 TERMINATED | 1 RUNNING | 3 PENDING\n",
      "Current time: 2025-03-26 14:59:00. Total running time: 24min 34s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00005   RUNNING      0.01              64                                                     |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10            386.866   0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00004   TERMINATED   0.01              32       10            290.39    0.377976    0.886378  |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=12902)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00005_5_batch_size=64,lr=0.0100_2025-03-26_14-34-26/checkpoint_000000)\n",
      "2025-03-26 14:59:17,498\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.231 s, which may be a performance bottleneck.\n",
      "2025-03-26 14:59:17,500\tWARNING util.py:201 -- The `process_trial_result` operation took 1.234 s, which may be a performance bottleneck.\n",
      "2025-03-26 14:59:17,506\tWARNING util.py:201 -- Processing trial results took 1.240 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-03-26 14:59:17,508\tWARNING util.py:201 -- The `process_trial_result` operation took 1.242 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial train_emnist_6b36b_00005 finished iteration 1 at 2025-03-26 14:59:17. Total running time: 24min 50s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00005 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000000 |\n",
      "| time_this_iter_s                                    27.87274 |\n",
      "| time_total_s                                        27.87274 |\n",
      "| training_iteration                                         1 |\n",
      "| accuracy                                             0.83113 |\n",
      "| loss                                      0.5625548362731934 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00005 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00005_5_batch_size=64,lr=0.0100_2025-03-26_14-34-26/checkpoint_000000\n",
      "\n",
      "Trial status: 5 TERMINATED | 1 RUNNING | 3 PENDING\n",
      "Current time: 2025-03-26 14:59:30. Total running time: 25min 4s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00005   RUNNING      0.01              64        1            27.8727   0.562555    0.83113   |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10           378.584    3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4           116.161    3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10           246.891    2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10           386.866    0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00004   TERMINATED   0.01              32       10           290.39     0.377976    0.886378  |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_emnist_6b36b_00005 finished iteration 2 at 2025-03-26 14:59:42. Total running time: 25min 16s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00005 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000001 |\n",
      "| time_this_iter_s                                    25.22763 |\n",
      "| time_total_s                                        53.10037 |\n",
      "| training_iteration                                         2 |\n",
      "| accuracy                                             0.86414 |\n",
      "| loss                                      0.4428924024105072 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00005 saved a checkpoint for iteration 2 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00005_5_batch_size=64,lr=0.0100_2025-03-26_14-34-26/checkpoint_000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=12902)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00005_5_batch_size=64,lr=0.0100_2025-03-26_14-34-26/checkpoint_000001)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 5 TERMINATED | 1 RUNNING | 3 PENDING\n",
      "Current time: 2025-03-26 15:00:00. Total running time: 25min 34s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00005   RUNNING      0.01              64        2            53.1004   0.442892    0.864143  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10           378.584    3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4           116.161    3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10           246.891    2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10           386.866    0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00004   TERMINATED   0.01              32       10           290.39     0.377976    0.886378  |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_emnist_6b36b_00005 finished iteration 3 at 2025-03-26 15:00:07. Total running time: 25min 41s\n",
      "+---------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00005 result                         |\n",
      "+---------------------------------------------------------------+\n",
      "| checkpoint_dir_name                         checkpoint_000002 |\n",
      "| time_this_iter_s                                     25.11885 |\n",
      "| time_total_s                                         78.21921 |\n",
      "| training_iteration                                          3 |\n",
      "| accuracy                                              0.87039 |\n",
      "| loss                                      0.41570666432380676 |\n",
      "+---------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00005 saved a checkpoint for iteration 3 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00005_5_batch_size=64,lr=0.0100_2025-03-26_14-34-26/checkpoint_000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=12902)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00005_5_batch_size=64,lr=0.0100_2025-03-26_14-34-26/checkpoint_000002)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 5 TERMINATED | 1 RUNNING | 3 PENDING\n",
      "Current time: 2025-03-26 15:00:30. Total running time: 26min 4s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00005   RUNNING      0.01              64        3            78.2192   0.415707    0.870393  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10           378.584    3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4           116.161    3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10           246.891    2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10           386.866    0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00004   TERMINATED   0.01              32       10           290.39     0.377976    0.886378  |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=12902)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00005_5_batch_size=64,lr=0.0100_2025-03-26_14-34-26/checkpoint_000003)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial train_emnist_6b36b_00005 finished iteration 4 at 2025-03-26 15:00:31. Total running time: 26min 4s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00005 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000003 |\n",
      "| time_this_iter_s                                    23.53214 |\n",
      "| time_total_s                                       101.75135 |\n",
      "| training_iteration                                         4 |\n",
      "| accuracy                                             0.88165 |\n",
      "| loss                                      0.3706710934638977 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00005 saved a checkpoint for iteration 4 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00005_5_batch_size=64,lr=0.0100_2025-03-26_14-34-26/checkpoint_000003\n",
      "\n",
      "Trial train_emnist_6b36b_00005 finished iteration 5 at 2025-03-26 15:00:56. Total running time: 26min 29s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00005 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000004 |\n",
      "| time_this_iter_s                                    25.02952 |\n",
      "| time_total_s                                       126.78087 |\n",
      "| training_iteration                                         5 |\n",
      "| accuracy                                             0.88297 |\n",
      "| loss                                      0.3746884763240814 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00005 saved a checkpoint for iteration 5 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00005_5_batch_size=64,lr=0.0100_2025-03-26_14-34-26/checkpoint_000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=12902)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00005_5_batch_size=64,lr=0.0100_2025-03-26_14-34-26/checkpoint_000004)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 5 TERMINATED | 1 RUNNING | 3 PENDING\n",
      "Current time: 2025-03-26 15:01:00. Total running time: 26min 34s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00005   RUNNING      0.01              64        5            126.781   0.374688    0.882973  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10            386.866   0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00004   TERMINATED   0.01              32       10            290.39    0.377976    0.886378  |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_emnist_6b36b_00005 finished iteration 6 at 2025-03-26 15:01:21. Total running time: 26min 54s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00005 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000005 |\n",
      "| time_this_iter_s                                     25.0496 |\n",
      "| time_total_s                                       151.83047 |\n",
      "| training_iteration                                         6 |\n",
      "| accuracy                                             0.88526 |\n",
      "| loss                                      0.3715546727180481 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00005 saved a checkpoint for iteration 6 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00005_5_batch_size=64,lr=0.0100_2025-03-26_14-34-26/checkpoint_000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=12902)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00005_5_batch_size=64,lr=0.0100_2025-03-26_14-34-26/checkpoint_000005)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 5 TERMINATED | 1 RUNNING | 3 PENDING\n",
      "Current time: 2025-03-26 15:01:30. Total running time: 27min 4s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00005   RUNNING      0.01              64        6            151.83    0.371555    0.885256  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10            386.866   0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00004   TERMINATED   0.01              32       10            290.39    0.377976    0.886378  |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_emnist_6b36b_00005 finished iteration 7 at 2025-03-26 15:01:46. Total running time: 27min 19s\n",
      "+---------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00005 result                         |\n",
      "+---------------------------------------------------------------+\n",
      "| checkpoint_dir_name                         checkpoint_000006 |\n",
      "| time_this_iter_s                                     24.66882 |\n",
      "| time_total_s                                        176.49929 |\n",
      "| training_iteration                                          7 |\n",
      "| accuracy                                              0.89002 |\n",
      "| loss                                      0.34829047322273254 |\n",
      "+---------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00005 saved a checkpoint for iteration 7 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00005_5_batch_size=64,lr=0.0100_2025-03-26_14-34-26/checkpoint_000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=12902)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00005_5_batch_size=64,lr=0.0100_2025-03-26_14-34-26/checkpoint_000006)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 5 TERMINATED | 1 RUNNING | 3 PENDING\n",
      "Current time: 2025-03-26 15:02:01. Total running time: 27min 34s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00005   RUNNING      0.01              64        7            176.499   0.34829     0.890024  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10            386.866   0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00004   TERMINATED   0.01              32       10            290.39    0.377976    0.886378  |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_emnist_6b36b_00005 finished iteration 8 at 2025-03-26 15:02:10. Total running time: 27min 43s\n",
      "+---------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00005 result                         |\n",
      "+---------------------------------------------------------------+\n",
      "| checkpoint_dir_name                         checkpoint_000007 |\n",
      "| time_this_iter_s                                     23.94536 |\n",
      "| time_total_s                                        200.44465 |\n",
      "| training_iteration                                          8 |\n",
      "| accuracy                                              0.88474 |\n",
      "| loss                                      0.36459237337112427 |\n",
      "+---------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00005 saved a checkpoint for iteration 8 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00005_5_batch_size=64,lr=0.0100_2025-03-26_14-34-26/checkpoint_000007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=12902)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00005_5_batch_size=64,lr=0.0100_2025-03-26_14-34-26/checkpoint_000007)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 5 TERMINATED | 1 RUNNING | 3 PENDING\n",
      "Current time: 2025-03-26 15:02:31. Total running time: 28min 4s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00005   RUNNING      0.01              64        8            200.445   0.364592    0.884736  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10            386.866   0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00004   TERMINATED   0.01              32       10            290.39    0.377976    0.886378  |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_emnist_6b36b_00005 finished iteration 9 at 2025-03-26 15:02:35. Total running time: 28min 8s\n",
      "+---------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00005 result                         |\n",
      "+---------------------------------------------------------------+\n",
      "| checkpoint_dir_name                         checkpoint_000008 |\n",
      "| time_this_iter_s                                     25.13349 |\n",
      "| time_total_s                                        225.57813 |\n",
      "| training_iteration                                          9 |\n",
      "| accuracy                                              0.89527 |\n",
      "| loss                                      0.34009531140327454 |\n",
      "+---------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00005 saved a checkpoint for iteration 9 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00005_5_batch_size=64,lr=0.0100_2025-03-26_14-34-26/checkpoint_000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=12902)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00005_5_batch_size=64,lr=0.0100_2025-03-26_14-34-26/checkpoint_000008)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial train_emnist_6b36b_00005 finished iteration 10 at 2025-03-26 15:03:00. Total running time: 28min 33s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00005 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000009 |\n",
      "| time_this_iter_s                                    25.27807 |\n",
      "| time_total_s                                        250.8562 |\n",
      "| training_iteration                                        10 |\n",
      "| accuracy                                             0.88578 |\n",
      "| loss                                      0.3651599884033203 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00005 saved a checkpoint for iteration 10 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00005_5_batch_size=64,lr=0.0100_2025-03-26_14-34-26/checkpoint_000009\n",
      "\n",
      "Trial train_emnist_6b36b_00005 completed after 10 iterations at 2025-03-26 15:03:00. Total running time: 28min 33s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=12902)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00005_5_batch_size=64,lr=0.0100_2025-03-26_14-34-26/checkpoint_000009)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 6 TERMINATED | 3 PENDING\n",
      "Current time: 2025-03-26 15:03:01. Total running time: 28min 34s\n",
      "Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10            386.866   0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00004   TERMINATED   0.01              32       10            290.39    0.377976    0.886378  |\n",
      "| train_emnist_6b36b_00005   TERMINATED   0.01              64       10            250.856   0.36516     0.885777  |\n",
      "| train_emnist_6b36b_00006   PENDING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb: uploading config.yaml\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb:                                                                                \n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb: Run history:\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb:                 accuracy ‚ñÅ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb: iterations_since_restore ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb:                     loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb:       time_since_restore ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb:         time_this_iter_s ‚ñà‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÑ\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb:             time_total_s ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb:                timestamp ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb:       training_iteration ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb: Run summary:\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb:                 accuracy 0.88578\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb: iterations_since_restore 10\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb:                     loss 0.36516\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb:       time_since_restore 250.8562\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb:         time_this_iter_s 25.27807\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb:             time_total_s 250.8562\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb:                timestamp 1743001380\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb:       training_iteration 10\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb: üöÄ View run train_emnist_6b36b_00005 at: https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2/runs/6b36b_00005\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb: ‚≠êÔ∏è View project at: https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[36m(_WandbLoggingActor pid=12961)\u001b[0m wandb: Find logs at: ./wandb/run-20250326_145854-6b36b_00005/logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial train_emnist_6b36b_00006 started with configuration:\n",
      "+-------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00006 config           |\n",
      "+-------------------------------------------------+\n",
      "| batch_size                                   16 |\n",
      "| lr                                        0.001 |\n",
      "+-------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb: Currently logged in as: shadaevf (shadaevf-rtu-mirea) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb: Tracking run with wandb version 0.19.8\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2025-03-26_14-34-21_606425_267/artifacts/2025-03-26_14-34-26/train_emnist_2025-03-26_14-34-26/driver_artifacts/train_emnist_6b36b_00006_6_batch_size=16,lr=0.0010_2025-03-26_14-34-26/wandb/run-20250326_150311-6b36b_00006\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb: Syncing run train_emnist_6b36b_00006\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb: ‚≠êÔ∏è View project at https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb: üöÄ View run at https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2/runs/6b36b_00006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=14320)\u001b[0m [1,  2000] loss: 1.609\n",
      "\u001b[36m(func pid=14320)\u001b[0m [1,  4000] loss: 0.533\n",
      "\n",
      "Trial status: 6 TERMINATED | 1 RUNNING | 2 PENDING\n",
      "Current time: 2025-03-26 15:03:31. Total running time: 29min 4s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00006   RUNNING      0.001             16                                                     |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10            386.866   0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00004   TERMINATED   0.01              32       10            290.39    0.377976    0.886378  |\n",
      "| train_emnist_6b36b_00005   TERMINATED   0.01              64       10            250.856   0.36516     0.885777  |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=14320)\u001b[0m [1,  6000] loss: 0.296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=14320)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00006_6_batch_size=16,lr=0.0010_2025-03-26_14-34-26/checkpoint_000000)\n",
      "2025-03-26 15:03:49,202\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.228 s, which may be a performance bottleneck.\n",
      "2025-03-26 15:03:49,204\tWARNING util.py:201 -- The `process_trial_result` operation took 1.231 s, which may be a performance bottleneck.\n",
      "2025-03-26 15:03:49,205\tWARNING util.py:201 -- Processing trial results took 1.232 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-03-26 15:03:49,205\tWARNING util.py:201 -- The `process_trial_result` operation took 1.232 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial train_emnist_6b36b_00006 finished iteration 1 at 2025-03-26 15:03:49. Total running time: 29min 22s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00006 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000000 |\n",
      "| time_this_iter_s                                    41.51766 |\n",
      "| time_total_s                                        41.51766 |\n",
      "| training_iteration                                         1 |\n",
      "| accuracy                                             0.77248 |\n",
      "| loss                                      0.7871807217597961 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00006 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00006_6_batch_size=16,lr=0.0010_2025-03-26_14-34-26/checkpoint_000000\n",
      "\u001b[36m(func pid=14320)\u001b[0m [2,  2000] loss: 0.735\n",
      "\n",
      "Trial status: 6 TERMINATED | 1 RUNNING | 2 PENDING\n",
      "Current time: 2025-03-26 15:04:01. Total running time: 29min 34s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00006   RUNNING      0.001             16        1            41.5177   0.787181    0.772476  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10           378.584    3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4           116.161    3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10           246.891    2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10           386.866    0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00004   TERMINATED   0.01              32       10           290.39     0.377976    0.886378  |\n",
      "| train_emnist_6b36b_00005   TERMINATED   0.01              64       10           250.856    0.36516     0.885777  |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=14320)\u001b[0m [2,  4000] loss: 0.330\n",
      "\u001b[36m(func pid=14320)\u001b[0m [2,  6000] loss: 0.200\n",
      "\n",
      "Trial train_emnist_6b36b_00006 finished iteration 2 at 2025-03-26 15:04:27. Total running time: 30min 0s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00006 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000001 |\n",
      "| time_this_iter_s                                    38.36752 |\n",
      "| time_total_s                                        79.88518 |\n",
      "| training_iteration                                         2 |\n",
      "| accuracy                                             0.83377 |\n",
      "| loss                                      0.5692867636680603 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00006 saved a checkpoint for iteration 2 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00006_6_batch_size=16,lr=0.0010_2025-03-26_14-34-26/checkpoint_000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=14320)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00006_6_batch_size=16,lr=0.0010_2025-03-26_14-34-26/checkpoint_000001)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 6 TERMINATED | 1 RUNNING | 2 PENDING\n",
      "Current time: 2025-03-26 15:04:31. Total running time: 30min 4s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00006   RUNNING      0.001             16        2            79.8852   0.569287    0.833774  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10           378.584    3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4           116.161    3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10           246.891    2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10           386.866    0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00004   TERMINATED   0.01              32       10           290.39     0.377976    0.886378  |\n",
      "| train_emnist_6b36b_00005   TERMINATED   0.01              64       10           250.856    0.36516     0.885777  |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=14320)\u001b[0m [3,  2000] loss: 0.537\n",
      "\u001b[36m(func pid=14320)\u001b[0m [3,  4000] loss: 0.252\n",
      "\u001b[36m(func pid=14320)\u001b[0m [3,  6000] loss: 0.162\n",
      "Trial status: 6 TERMINATED | 1 RUNNING | 2 PENDING\n",
      "Current time: 2025-03-26 15:05:01. Total running time: 30min 34s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00006   RUNNING      0.001             16        2            79.8852   0.569287    0.833774  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10           378.584    3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4           116.161    3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10           246.891    2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10           386.866    0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00004   TERMINATED   0.01              32       10           290.39     0.377976    0.886378  |\n",
      "| train_emnist_6b36b_00005   TERMINATED   0.01              64       10           250.856    0.36516     0.885777  |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_emnist_6b36b_00006 finished iteration 3 at 2025-03-26 15:05:04. Total running time: 30min 38s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00006 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000002 |\n",
      "| time_this_iter_s                                    37.34977 |\n",
      "| time_total_s                                       117.23495 |\n",
      "| training_iteration                                         3 |\n",
      "| accuracy                                             0.85933 |\n",
      "| loss                                      0.4712809920310974 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00006 saved a checkpoint for iteration 3 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00006_6_batch_size=16,lr=0.0010_2025-03-26_14-34-26/checkpoint_000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=14320)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00006_6_batch_size=16,lr=0.0010_2025-03-26_14-34-26/checkpoint_000002)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=14320)\u001b[0m [4,  2000] loss: 0.457\n",
      "\u001b[36m(func pid=14320)\u001b[0m [4,  4000] loss: 0.217\n",
      "\n",
      "Trial status: 6 TERMINATED | 1 RUNNING | 2 PENDING\n",
      "Current time: 2025-03-26 15:05:31. Total running time: 31min 4s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00006   RUNNING      0.001             16        3            117.235   0.471281    0.859335  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10            386.866   0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00004   TERMINATED   0.01              32       10            290.39    0.377976    0.886378  |\n",
      "| train_emnist_6b36b_00005   TERMINATED   0.01              64       10            250.856   0.36516     0.885777  |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=14320)\u001b[0m [4,  6000] loss: 0.139\n",
      "\n",
      "Trial train_emnist_6b36b_00006 finished iteration 4 at 2025-03-26 15:05:42. Total running time: 31min 16s\n",
      "+---------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00006 result                         |\n",
      "+---------------------------------------------------------------+\n",
      "| checkpoint_dir_name                         checkpoint_000003 |\n",
      "| time_this_iter_s                                      37.7437 |\n",
      "| time_total_s                                        154.97866 |\n",
      "| training_iteration                                          4 |\n",
      "| accuracy                                              0.86791 |\n",
      "| loss                                      0.43824440240859985 |\n",
      "+---------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00006 saved a checkpoint for iteration 4 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00006_6_batch_size=16,lr=0.0010_2025-03-26_14-34-26/checkpoint_000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=14320)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00006_6_batch_size=16,lr=0.0010_2025-03-26_14-34-26/checkpoint_000003)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=14320)\u001b[0m [5,  2000] loss: 0.401\n",
      "\n",
      "Trial status: 6 TERMINATED | 1 RUNNING | 2 PENDING\n",
      "Current time: 2025-03-26 15:06:01. Total running time: 31min 34s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00006   RUNNING      0.001             16        4            154.979   0.438244    0.867909  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10            386.866   0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00004   TERMINATED   0.01              32       10            290.39    0.377976    0.886378  |\n",
      "| train_emnist_6b36b_00005   TERMINATED   0.01              64       10            250.856   0.36516     0.885777  |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=14320)\u001b[0m [5,  4000] loss: 0.193\n",
      "\u001b[36m(func pid=14320)\u001b[0m [5,  6000] loss: 0.128\n",
      "\n",
      "Trial train_emnist_6b36b_00006 finished iteration 5 at 2025-03-26 15:06:21. Total running time: 31min 54s\n",
      "+---------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00006 result                         |\n",
      "+---------------------------------------------------------------+\n",
      "| checkpoint_dir_name                         checkpoint_000004 |\n",
      "| time_this_iter_s                                     38.30255 |\n",
      "| time_total_s                                        193.28121 |\n",
      "| training_iteration                                          5 |\n",
      "| accuracy                                              0.88077 |\n",
      "| loss                                      0.39170610904693604 |\n",
      "+---------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00006 saved a checkpoint for iteration 5 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00006_6_batch_size=16,lr=0.0010_2025-03-26_14-34-26/checkpoint_000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=14320)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00006_6_batch_size=16,lr=0.0010_2025-03-26_14-34-26/checkpoint_000004)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=14320)\u001b[0m [6,  2000] loss: 0.361\n",
      "\n",
      "Trial status: 6 TERMINATED | 1 RUNNING | 2 PENDING\n",
      "Current time: 2025-03-26 15:06:31. Total running time: 32min 4s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00006   RUNNING      0.001             16        5            193.281   0.391706    0.880769  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10            386.866   0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00004   TERMINATED   0.01              32       10            290.39    0.377976    0.886378  |\n",
      "| train_emnist_6b36b_00005   TERMINATED   0.01              64       10            250.856   0.36516     0.885777  |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=14320)\u001b[0m [6,  4000] loss: 0.179\n",
      "\u001b[36m(func pid=14320)\u001b[0m [6,  6000] loss: 0.119\n",
      "\n",
      "Trial train_emnist_6b36b_00006 finished iteration 6 at 2025-03-26 15:06:58. Total running time: 32min 32s\n",
      "+-------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00006 result                       |\n",
      "+-------------------------------------------------------------+\n",
      "| checkpoint_dir_name                       checkpoint_000005 |\n",
      "| time_this_iter_s                                   37.86644 |\n",
      "| time_total_s                                      231.14765 |\n",
      "| training_iteration                                        6 |\n",
      "| accuracy                                            0.88654 |\n",
      "| loss                                      0.367011696100235 |\n",
      "+-------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00006 saved a checkpoint for iteration 6 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00006_6_batch_size=16,lr=0.0010_2025-03-26_14-34-26/checkpoint_000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=14320)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00006_6_batch_size=16,lr=0.0010_2025-03-26_14-34-26/checkpoint_000005)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 6 TERMINATED | 1 RUNNING | 2 PENDING\n",
      "Current time: 2025-03-26 15:07:01. Total running time: 32min 35s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00006   RUNNING      0.001             16        6            231.148   0.367012    0.886538  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10            386.866   0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00004   TERMINATED   0.01              32       10            290.39    0.377976    0.886378  |\n",
      "| train_emnist_6b36b_00005   TERMINATED   0.01              64       10            250.856   0.36516     0.885777  |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=14320)\u001b[0m [7,  2000] loss: 0.340\n",
      "\u001b[36m(func pid=14320)\u001b[0m [7,  4000] loss: 0.169\n",
      "\u001b[36m(func pid=14320)\u001b[0m [7,  6000] loss: 0.108\n",
      "Trial status: 6 TERMINATED | 1 RUNNING | 2 PENDING\n",
      "Current time: 2025-03-26 15:07:31. Total running time: 33min 5s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00006   RUNNING      0.001             16        6            231.148   0.367012    0.886538  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10            386.866   0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00004   TERMINATED   0.01              32       10            290.39    0.377976    0.886378  |\n",
      "| train_emnist_6b36b_00005   TERMINATED   0.01              64       10            250.856   0.36516     0.885777  |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Trial train_emnist_6b36b_00006 finished iteration 7 at 2025-03-26 15:07:36. Total running time: 33min 10s\n",
      "+-------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00006 result                       |\n",
      "+-------------------------------------------------------------+\n",
      "| checkpoint_dir_name                       checkpoint_000006 |\n",
      "| time_this_iter_s                                   37.78175 |\n",
      "| time_total_s                                       268.9294 |\n",
      "| training_iteration                                        7 |\n",
      "| accuracy                                            0.88846 |\n",
      "| loss                                      0.361369252204895 |\n",
      "+-------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00006 saved a checkpoint for iteration 7 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00006_6_batch_size=16,lr=0.0010_2025-03-26_14-34-26/checkpoint_000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=14320)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00006_6_batch_size=16,lr=0.0010_2025-03-26_14-34-26/checkpoint_000006)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=14320)\u001b[0m [8,  2000] loss: 0.315\n",
      "\u001b[36m(func pid=14320)\u001b[0m [8,  4000] loss: 0.158\n",
      "\n",
      "Trial status: 6 TERMINATED | 1 RUNNING | 2 PENDING\n",
      "Current time: 2025-03-26 15:08:01. Total running time: 33min 35s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00006   RUNNING      0.001             16        7            268.929   0.361369    0.888462  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10            386.866   0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00004   TERMINATED   0.01              32       10            290.39    0.377976    0.886378  |\n",
      "| train_emnist_6b36b_00005   TERMINATED   0.01              64       10            250.856   0.36516     0.885777  |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=14320)\u001b[0m [8,  6000] loss: 0.106\n",
      "\n",
      "Trial train_emnist_6b36b_00006 finished iteration 8 at 2025-03-26 15:08:15. Total running time: 33min 48s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00006 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000007 |\n",
      "| time_this_iter_s                                    38.60738 |\n",
      "| time_total_s                                       307.53678 |\n",
      "| training_iteration                                         8 |\n",
      "| accuracy                                             0.89383 |\n",
      "| loss                                      0.3497985005378723 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00006 saved a checkpoint for iteration 8 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00006_6_batch_size=16,lr=0.0010_2025-03-26_14-34-26/checkpoint_000007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=14320)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00006_6_batch_size=16,lr=0.0010_2025-03-26_14-34-26/checkpoint_000007)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=14320)\u001b[0m [9,  2000] loss: 0.299\n",
      "\n",
      "Trial status: 6 TERMINATED | 1 RUNNING | 2 PENDING\n",
      "Current time: 2025-03-26 15:08:31. Total running time: 34min 5s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00006   RUNNING      0.001             16        8            307.537   0.349799    0.89383   |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10            386.866   0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00004   TERMINATED   0.01              32       10            290.39    0.377976    0.886378  |\n",
      "| train_emnist_6b36b_00005   TERMINATED   0.01              64       10            250.856   0.36516     0.885777  |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=14320)\u001b[0m [9,  4000] loss: 0.152\n",
      "\u001b[36m(func pid=14320)\u001b[0m [9,  6000] loss: 0.098\n",
      "\n",
      "Trial train_emnist_6b36b_00006 finished iteration 9 at 2025-03-26 15:08:53. Total running time: 34min 27s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00006 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000008 |\n",
      "| time_this_iter_s                                    38.53107 |\n",
      "| time_total_s                                       346.06785 |\n",
      "| training_iteration                                         9 |\n",
      "| accuracy                                             0.89415 |\n",
      "| loss                                      0.3411262631416321 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00006 saved a checkpoint for iteration 9 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00006_6_batch_size=16,lr=0.0010_2025-03-26_14-34-26/checkpoint_000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=14320)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00006_6_batch_size=16,lr=0.0010_2025-03-26_14-34-26/checkpoint_000008)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 6 TERMINATED | 1 RUNNING | 2 PENDING\n",
      "Current time: 2025-03-26 15:09:01. Total running time: 34min 35s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00006   RUNNING      0.001             16        9            346.068   0.341126    0.894151  |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10            386.866   0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00004   TERMINATED   0.01              32       10            290.39    0.377976    0.886378  |\n",
      "| train_emnist_6b36b_00005   TERMINATED   0.01              64       10            250.856   0.36516     0.885777  |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[36m(func pid=14320)\u001b[0m [10,  2000] loss: 0.286\n",
      "\u001b[36m(func pid=14320)\u001b[0m [10,  4000] loss: 0.143\n",
      "\u001b[36m(func pid=14320)\u001b[0m [10,  6000] loss: 0.095\n",
      "\n",
      "Trial train_emnist_6b36b_00006 finished iteration 10 at 2025-03-26 15:09:30. Total running time: 35min 4s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00006 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000009 |\n",
      "| time_this_iter_s                                     37.1051 |\n",
      "| time_total_s                                       383.17296 |\n",
      "| training_iteration                                        10 |\n",
      "| accuracy                                              0.8982 |\n",
      "| loss                                      0.3281926214694977 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00006 saved a checkpoint for iteration 10 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00006_6_batch_size=16,lr=0.0010_2025-03-26_14-34-26/checkpoint_000009\n",
      "\n",
      "Trial train_emnist_6b36b_00006 completed after 10 iterations at 2025-03-26 15:09:30. Total running time: 35min 4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=14320)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00006_6_batch_size=16,lr=0.0010_2025-03-26_14-34-26/checkpoint_000009)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 7 TERMINATED | 2 PENDING\n",
      "Current time: 2025-03-26 15:09:31. Total running time: 35min 5s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10            386.866   0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00004   TERMINATED   0.01              32       10            290.39    0.377976    0.886378  |\n",
      "| train_emnist_6b36b_00005   TERMINATED   0.01              64       10            250.856   0.36516     0.885777  |\n",
      "| train_emnist_6b36b_00006   TERMINATED   0.001             16       10            383.173   0.328193    0.898197  |\n",
      "| train_emnist_6b36b_00007   PENDING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb: uploading config.yaml\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb:                                                                                \n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb: Run history:\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb:                 accuracy ‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb: iterations_since_restore ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb:                     loss ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb:       time_since_restore ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb:         time_this_iter_s ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb:             time_total_s ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb:                timestamp ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb:       training_iteration ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb: Run summary:\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb:                 accuracy 0.8982\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb: iterations_since_restore 10\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb:                     loss 0.32819\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb:       time_since_restore 383.17296\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb:         time_this_iter_s 37.1051\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb:             time_total_s 383.17296\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb:                timestamp 1743001770\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb:       training_iteration 10\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb: üöÄ View run train_emnist_6b36b_00006 at: https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2/runs/6b36b_00006\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb: ‚≠êÔ∏è View project at: https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[36m(_WandbLoggingActor pid=14376)\u001b[0m wandb: Find logs at: ./wandb/run-20250326_150311-6b36b_00006/logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial train_emnist_6b36b_00007 started with configuration:\n",
      "+-------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00007 config           |\n",
      "+-------------------------------------------------+\n",
      "| batch_size                                   32 |\n",
      "| lr                                        0.001 |\n",
      "+-------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb: Currently logged in as: shadaevf (shadaevf-rtu-mirea) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb: Tracking run with wandb version 0.19.8\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2025-03-26_14-34-21_606425_267/artifacts/2025-03-26_14-34-26/train_emnist_2025-03-26_14-34-26/driver_artifacts/train_emnist_6b36b_00007_7_batch_size=32,lr=0.0010_2025-03-26_14-34-26/wandb/run-20250326_150941-6b36b_00007\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb: Syncing run train_emnist_6b36b_00007\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb: ‚≠êÔ∏è View project at https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb: üöÄ View run at https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2/runs/6b36b_00007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=16289)\u001b[0m [1,  2000] loss: 1.539\n",
      "\n",
      "Trial status: 7 TERMINATED | 1 RUNNING | 1 PENDING\n",
      "Current time: 2025-03-26 15:10:02. Total running time: 35min 35s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00007   RUNNING      0.001             32                                                     |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10            378.584   3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4            116.161   3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10            246.891   2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10            386.866   0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00004   TERMINATED   0.01              32       10            290.39    0.377976    0.886378  |\n",
      "| train_emnist_6b36b_00005   TERMINATED   0.01              64       10            250.856   0.36516     0.885777  |\n",
      "| train_emnist_6b36b_00006   TERMINATED   0.001             16       10            383.173   0.328193    0.898197  |\n",
      "| train_emnist_6b36b_00008   PENDING      0.001             64                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=16289)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00007_7_batch_size=32,lr=0.0010_2025-03-26_14-34-26/checkpoint_000000)\n",
      "2025-03-26 15:10:10,370\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.250 s, which may be a performance bottleneck.\n",
      "2025-03-26 15:10:10,372\tWARNING util.py:201 -- The `process_trial_result` operation took 1.253 s, which may be a performance bottleneck.\n",
      "2025-03-26 15:10:10,373\tWARNING util.py:201 -- Processing trial results took 1.253 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-03-26 15:10:10,374\tWARNING util.py:201 -- The `process_trial_result` operation took 1.254 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial train_emnist_6b36b_00007 finished iteration 1 at 2025-03-26 15:10:10. Total running time: 35min 43s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00007 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000000 |\n",
      "| time_this_iter_s                                    31.70022 |\n",
      "| time_total_s                                        31.70022 |\n",
      "| training_iteration                                         1 |\n",
      "| accuracy                                             0.70044 |\n",
      "| loss                                      1.0339161157608032 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00007 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00007_7_batch_size=32,lr=0.0010_2025-03-26_14-34-26/checkpoint_000000\n",
      "\n",
      "Trial train_emnist_6b36b_00007 completed after 1 iterations at 2025-03-26 15:10:10. Total running time: 35min 43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb: uploading config.yaml\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb:                                                                                \n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb: Run history:\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb:                 accuracy ‚ñÅ\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb: iterations_since_restore ‚ñÅ\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb:                     loss ‚ñÅ\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb:       time_since_restore ‚ñÅ\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb:         time_this_iter_s ‚ñÅ\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb:             time_total_s ‚ñÅ\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb:                timestamp ‚ñÅ\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb:       training_iteration ‚ñÅ\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb: Run summary:\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb:                 accuracy 0.70044\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb:                     loss 1.03392\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb:       time_since_restore 31.70022\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb:         time_this_iter_s 31.70022\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb:             time_total_s 31.70022\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb:                timestamp 1743001809\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb: üöÄ View run train_emnist_6b36b_00007 at: https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2/runs/6b36b_00007\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb: ‚≠êÔ∏è View project at: https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[36m(_WandbLoggingActor pid=16353)\u001b[0m wandb: Find logs at: ./wandb/run-20250326_150941-6b36b_00007/logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial train_emnist_6b36b_00008 started with configuration:\n",
      "+-------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00008 config           |\n",
      "+-------------------------------------------------+\n",
      "| batch_size                                   64 |\n",
      "| lr                                        0.001 |\n",
      "+-------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb: Currently logged in as: shadaevf (shadaevf-rtu-mirea) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb: Tracking run with wandb version 0.19.8\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2025-03-26_14-34-21_606425_267/artifacts/2025-03-26_14-34-26/train_emnist_2025-03-26_14-34-26/driver_artifacts/train_emnist_6b36b_00008_8_batch_size=64,lr=0.0010_2025-03-26_14-34-26/wandb/run-20250326_151021-6b36b_00008\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb: Syncing run train_emnist_6b36b_00008\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb: ‚≠êÔ∏è View project at https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb: üöÄ View run at https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2/runs/6b36b_00008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial status: 8 TERMINATED | 1 RUNNING\n",
      "Current time: 2025-03-26 15:10:32. Total running time: 36min 5s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00008   RUNNING      0.001             64                                                     |\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10           378.584    3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4           116.161    3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10           246.891    2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10           386.866    0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00004   TERMINATED   0.01              32       10           290.39     0.377976    0.886378  |\n",
      "| train_emnist_6b36b_00005   TERMINATED   0.01              64       10           250.856    0.36516     0.885777  |\n",
      "| train_emnist_6b36b_00006   TERMINATED   0.001             16       10           383.173    0.328193    0.898197  |\n",
      "| train_emnist_6b36b_00007   TERMINATED   0.001             32        1            31.7002   1.03392     0.700441  |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(func pid=16642)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00008_8_batch_size=64,lr=0.0010_2025-03-26_14-34-26/checkpoint_000000)\n",
      "2025-03-26 15:10:45,439\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.798 s, which may be a performance bottleneck.\n",
      "2025-03-26 15:10:45,442\tWARNING util.py:201 -- The `process_trial_result` operation took 1.802 s, which may be a performance bottleneck.\n",
      "2025-03-26 15:10:45,445\tWARNING util.py:201 -- Processing trial results took 1.805 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-03-26 15:10:45,450\tWARNING util.py:201 -- The `process_trial_result` operation took 1.810 s, which may be a performance bottleneck.\n",
      "2025-03-26 15:10:45,511\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/train_emnist_2025-03-26_14-34-26' in 0.0505s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial train_emnist_6b36b_00008 finished iteration 1 at 2025-03-26 15:10:45. Total running time: 36min 18s\n",
      "+--------------------------------------------------------------+\n",
      "| Trial train_emnist_6b36b_00008 result                        |\n",
      "+--------------------------------------------------------------+\n",
      "| checkpoint_dir_name                        checkpoint_000000 |\n",
      "| time_this_iter_s                                    26.67169 |\n",
      "| time_total_s                                        26.67169 |\n",
      "| training_iteration                                         1 |\n",
      "| accuracy                                             0.65869 |\n",
      "| loss                                      1.1793996095657349 |\n",
      "+--------------------------------------------------------------+\n",
      "Trial train_emnist_6b36b_00008 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_emnist_2025-03-26_14-34-26/train_emnist_6b36b_00008_8_batch_size=64,lr=0.0010_2025-03-26_14-34-26/checkpoint_000000\n",
      "\n",
      "Trial train_emnist_6b36b_00008 completed after 1 iterations at 2025-03-26 15:10:45. Total running time: 36min 18s\n",
      "\n",
      "Trial status: 9 TERMINATED\n",
      "Current time: 2025-03-26 15:10:45. Total running time: 36min 18s\n",
      "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                 status          lr     batch_size     iter     total time (s)       loss     accuracy |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "| train_emnist_6b36b_00000   TERMINATED   0.1               16       10           378.584    3.27884     0.0369792 |\n",
      "| train_emnist_6b36b_00001   TERMINATED   0.1               32        4           116.161    3.26794     0.0384215 |\n",
      "| train_emnist_6b36b_00002   TERMINATED   0.1               64       10           246.891    2.80796     0.165064  |\n",
      "| train_emnist_6b36b_00003   TERMINATED   0.01              16       10           386.866    0.536529    0.854607  |\n",
      "| train_emnist_6b36b_00004   TERMINATED   0.01              32       10           290.39     0.377976    0.886378  |\n",
      "| train_emnist_6b36b_00005   TERMINATED   0.01              64       10           250.856    0.36516     0.885777  |\n",
      "| train_emnist_6b36b_00006   TERMINATED   0.001             16       10           383.173    0.328193    0.898197  |\n",
      "| train_emnist_6b36b_00007   TERMINATED   0.001             32        1            31.7002   1.03392     0.700441  |\n",
      "| train_emnist_6b36b_00008   TERMINATED   0.001             64        1            26.6717   1.1794      0.658694  |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Best trial config: {'lr': 0.001, 'batch_size': 16}\n",
      "Best trial final validation loss: 0.3281926214694977\n",
      "Best trial final validation accuracy: 0.8981971153846153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb: uploading history steps 0-0, summary\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb:                                                                                \n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb: Run history:\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb:                 accuracy ‚ñÅ\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb: iterations_since_restore ‚ñÅ\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb:                     loss ‚ñÅ\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb:       time_since_restore ‚ñÅ\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb:         time_this_iter_s ‚ñÅ\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb:             time_total_s ‚ñÅ\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb:                timestamp ‚ñÅ\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb:       training_iteration ‚ñÅ\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb: Run summary:\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb:                 accuracy 0.65869\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb:                     loss 1.1794\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb:       time_since_restore 26.67169\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb:         time_this_iter_s 26.67169\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb:             time_total_s 26.67169\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb:                timestamp 1743001843\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb: \n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb: üöÄ View run train_emnist_6b36b_00008 at: https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2/runs/6b36b_00008\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb: ‚≠êÔ∏è View project at: https://wandb.ai/shadaevf-rtu-mirea/ML2_2_Part_2\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[36m(_WandbLoggingActor pid=16701)\u001b[0m wandb: Find logs at: ./wandb/run-20250326_151021-6b36b_00008/logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial test set accuracy: 0.8920673076923077\n"
     ]
    }
   ],
   "source": [
    "# –°–∞–º –ø—Ä–æ—Ü–µ—Å—Å —Ç—é–Ω–∏–Ω–≥–∞\n",
    "def main(num_samples=10, max_num_epochs=10):\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    load_data(data_dir)\n",
    "    config = {\n",
    "\n",
    "        \"lr\": tune.grid_search([0.1, 0.01, 0.001]),\n",
    "        \"batch_size\": tune.grid_search([16, 32, 64]),\n",
    "    }\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2,\n",
    "    )\n",
    "    results = tune.run(\n",
    "        partial(train_emnist, data_dir=data_dir),\n",
    "        resources_per_trial={\"cpu\":2, \"gpu\": 1},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        callbacks=[WandbLoggerCallback(project=\"ML2_2_Part_2\")]\n",
    "\n",
    "    )\n",
    "\n",
    "    best_trial = results.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(f\"Best trial config: {best_trial.config}\")\n",
    "    print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "    print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n",
    "\n",
    "    best_trained_model = NeuralNetwork()\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    best_checkpoint = results.get_best_checkpoint(trial=best_trial, metric=\"accuracy\", mode=\"max\")\n",
    "    with best_checkpoint.as_directory() as checkpoint_dir:\n",
    "        data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "        with open(data_path, \"rb\") as fp:\n",
    "            best_checkpoint_data = pickle.load(fp)\n",
    "\n",
    "        best_trained_model.load_state_dict(best_checkpoint_data[\"net_state_dict\"])\n",
    "        test_acc = test_accuracy(best_trained_model, device)\n",
    "        print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # You can change the number of GPUs per trial here:\n",
    "    main(num_samples=1, max_num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 64,
     "status": "ok",
     "timestamp": 1743012850866,
     "user": {
      "displayName": "–®–∞–¥–∞–µ–≤ –§—ë–¥–æ—Ä",
      "userId": "14152552676221368510"
     },
     "user_tz": -180
    },
    "id": "bUcQ8Bd2y6_g"
   },
   "outputs": [],
   "source": [
    "def train_normal(config, data_dir=None):\n",
    "  # –°–æ–∑–¥–∞—ë–º —Å–µ—Ç—å\n",
    "  net = NeuralNetwork()\n",
    "  # –í—ã–±–∏—Ä–∞–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
    "  device = \"cpu\"\n",
    "  if torch.cuda.is_available():\n",
    "      device = \"cuda\"\n",
    "  net.to(device)\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.RMSprop(net.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "  trainset, testset = load_data(data_dir)\n",
    "\n",
    "  trainloader = torch.utils.data.DataLoader(\n",
    "      trainset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=2\n",
    "  )\n",
    "  testloader = torch.utils.data.DataLoader(\n",
    "      testset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=2\n",
    "  )\n",
    "\n",
    "  for epoch in range(10):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(\n",
    "                    \"[%d, %5d] loss: %.3f\"\n",
    "                    % (epoch + 1, i + 1, running_loss / epoch_steps)\n",
    "                )\n",
    "                running_loss = 0.0\n",
    "            wandb.log({\"train/loss\": loss})\n",
    "\n",
    "        # test loss\n",
    "        test_loss = 0.0\n",
    "        test_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.cpu().numpy()\n",
    "                test_steps += 1\n",
    "\n",
    "        wandb.log({\"test/loss\": test_loss / test_steps, \"test/accuracy\": correct / total})\n",
    "\n",
    "  print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 492977,
     "status": "ok",
     "timestamp": 1743013347810,
     "user": {
      "displayName": "–®–∞–¥–∞–µ–≤ –§—ë–¥–æ—Ä",
      "userId": "14152552676221368510"
     },
     "user_tz": -180
    },
    "id": "DCGVNe_HXZyb",
    "outputId": "b5615594-7dfe-4bed-86a5-317ab2103987"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250326_181415-2atg72n6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shadaevf-rtu-mirea/ml2_2_part_3/runs/2atg72n6' target=\"_blank\">RMSprop</a></strong> to <a href='https://wandb.ai/shadaevf-rtu-mirea/ml2_2_part_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shadaevf-rtu-mirea/ml2_2_part_3' target=\"_blank\">https://wandb.ai/shadaevf-rtu-mirea/ml2_2_part_3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shadaevf-rtu-mirea/ml2_2_part_3/runs/2atg72n6' target=\"_blank\">https://wandb.ai/shadaevf-rtu-mirea/ml2_2_part_3/runs/2atg72n6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.133\n",
      "[1,  4000] loss: 0.354\n",
      "[1,  6000] loss: 0.203\n",
      "[2,  2000] loss: 0.522\n",
      "[2,  4000] loss: 0.257\n",
      "[2,  6000] loss: 0.171\n",
      "[3,  2000] loss: 0.461\n",
      "[3,  4000] loss: 0.238\n",
      "[3,  6000] loss: 0.159\n",
      "[4,  2000] loss: 0.442\n",
      "[4,  4000] loss: 0.228\n",
      "[4,  6000] loss: 0.156\n",
      "[5,  2000] loss: 0.438\n",
      "[5,  4000] loss: 0.221\n",
      "[5,  6000] loss: 0.151\n",
      "[6,  2000] loss: 0.420\n",
      "[6,  4000] loss: 0.223\n",
      "[6,  6000] loss: 0.148\n",
      "[7,  2000] loss: 0.411\n",
      "[7,  4000] loss: 0.217\n",
      "[7,  6000] loss: 0.149\n",
      "[8,  2000] loss: 0.416\n",
      "[8,  4000] loss: 0.214\n",
      "[8,  6000] loss: 0.146\n",
      "[9,  2000] loss: 0.401\n",
      "[9,  4000] loss: 0.216\n",
      "[9,  6000] loss: 0.144\n",
      "[10,  2000] loss: 0.408\n",
      "[10,  4000] loss: 0.219\n",
      "[10,  6000] loss: 0.145\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test/accuracy</td><td>‚ñÅ‚ñÅ‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñà</td></tr><tr><td>test/loss</td><td>‚ñÖ‚ñÑ‚ñÅ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÉ‚ñà‚ñÑ</td></tr><tr><td>train/loss</td><td>‚ñÜ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñà‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÖ‚ñÇ‚ñÅ‚ñÜ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test/accuracy</td><td>0.86183</td></tr><tr><td>test/loss</td><td>0.53552</td></tr><tr><td>train/loss</td><td>0.38444</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">RMSprop</strong> at: <a href='https://wandb.ai/shadaevf-rtu-mirea/ml2_2_part_3/runs/2atg72n6' target=\"_blank\">https://wandb.ai/shadaevf-rtu-mirea/ml2_2_part_3/runs/2atg72n6</a><br> View project at: <a href='https://wandb.ai/shadaevf-rtu-mirea/ml2_2_part_3' target=\"_blank\">https://wandb.ai/shadaevf-rtu-mirea/ml2_2_part_3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250326_181415-2atg72n6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "      # Set the project where this run will be logged\n",
    "      project=\"ml2_2_part_3\",\n",
    "      # We pass a run name (otherwise it‚Äôll be randomly assigned, like sunshine-lollypop-10)\n",
    "      name=\"RMSprop\"\n",
    "      )\n",
    "data_dir = os.path.abspath(\"./data\")\n",
    "train_normal(config={\"lr\":0.001, \"batch_size\":16}, data_dir = data_dir)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNvEoH0lFw1M9W8rto3AIAd",
   "collapsed_sections": [
    "sye8ukc4STIc"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
